from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM, get_peft_config, LoraModel
from transformers import AutoTokenizer, LlamaTokenizer, AutoModelForCausalLM, LlamaForCausalLM
import torch
import json
import os

# structure: Peft model (Qlora) > Lora model (base_model)  > Normal model > Headless version of normal model
# ohmodel: PeftModelForCausalLM             > LoraModel > LlamaForCausalLM          > LlamaModel
# mhmodel: MultiHeadPeftModelForCausalLM    > LoraModel > MultiheadLlamaForCausalLM > LlamaModel

class MultiheadLlamaForCausalLM(LlamaForCausalLM):
    def __init__(self, model, heads, config):
        print('here')
        super().__init__(config['llamamodel_config'])
        self.model = model
        self.heads = heads
        
    
    def from_other(othermodel, orighead, masks, vocabs, config):
        assert len(masks) == len(vocabs)
        return MultiheadLlamaForCausalLM(othermodel, [orighead], config)
    
    def _add_head(self, orighead):
        """in features and out features the same, but a mask will be applied at prediction.
        Basically just a method to deep-copy the original head"""
        in_features, out_features, bias = orighead.in_features, orighead.out_features, orighead.bias
        newhead = type(orighead)(in_features, out_features, bias)
        newhead.load_state_dict(orighead.state_dict())
        self.heads.append(newhead)
    
    def generate(self):
        print('success')

class MultiHeadPeftModelForCausalLM(PeftModelForCausalLM):
    def __init__(self, config, base_model):
        super().__init__(base_model, config) #sets self.base_model to base_model
        self.config = config
    
    def from_one_head(peft_model, head_masks, head_max_tokens, *args, **kwargs):
        """peft_model: A PeftModelForCausalLM,
            head_masks: a list of masks for head output vocabulary,
            head_max_tokens: a list of ints, where the nth element is the 
                        max number of new tokens to be generated by the nth head"""
        headless_model = peft_model.base_model.model.model #LlamaModel
        orighead = peft_model.base_model.model.lm_head
        normalmodel_config = {} 
        normalmodel_config['llamamodel_config'] = peft_model.base_model.model.config
        
        normalmodel = MultiheadLlamaForCausalLM.from_other(headless_model, orighead, [], [], normalmodel_config) # replaces LlamaForCausalLM
        # peft_model.base_model = normalmodel
        
        mhmodel = MultiHeadPeftModelForCausalLM(peft_model.peft_config['default'], normalmodel)
        return mhmodel
        
    def generate(self, *args, **kwargs):
        self.base_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation
        if hasattr(self.base_model, "model"):
            self.base_model.model.generation_config = self.generation_config
        else:
            self.base_model.generation_config = self.generation_config
            
        print(args, kwargs)
        outputs = self.base_model.forward(*args)
        
        