from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM, get_peft_config, LoraModel
from transformers import AutoTokenizer, LlamaTokenizer, AutoModelForCausalLM
import torch
import json
import os

class HeadlessLoraModel(LoraModel):
    def __init__(self, othermodel):
        super().__init__(model, config, adapter_name)
        
    
    def from_other(othermodel):
        return HeadlessLoraModel(othermodel)
    
    def generate():
        pass

class MultiHeadPeftModelForCausalLM(PeftModelForCausalLM):
    def __init__(self, config, model, heads, head_masks):
        super().__init__(model, config) #sets self.base_model to model
        self.config = config
        self.heads = heads
        self.head_masks = head_masks
    
    def from_one_head(peft_model, head_masks, head_max_tokens, *args, **kwargs):
        """peft_model: A PeftModelForCausalLM,
            head_masks: a list of masks for head output vocabulary,
            head_max_tokens: a list of ints, where the nth element is the 
                        max number of new tokens to be generated by the nth head"""
        model = list(peft_model.model.children())[0]
        # model = HeadlessLoraModel(model)
        orighead = list(peft_model.model.children())[1]
        mhmodel = MultiHeadPeftModelForCausalLM(peft_model.peft_config['default'], model, [], [])
        assert len(head_masks) == len(head_max_tokens)
        for i in range(len(head_masks)):
            mhmodel._add_head(orighead)
        return mhmodel
    
    def _add_head(self, orighead):
        """in features and out features the same, but a mask will be applied at prediction.
        Basically just a method to deep-copy the original head"""
        in_features, out_features, bias = orighead.in_features, orighead.out_features, orighead.bias
        newhead = type(orighead)(in_features, out_features, bias)
        newhead.load_state_dict(orighead.state_dict())
        self.heads.append(newhead)
        
    def generate(self, *args, **kwargs):
        self.base_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation
        if hasattr(self.base_model, "model"):
            self.base_model.model.generation_config = self.generation_config
        else:
            self.base_model.generation_config = self.generation_config
            
        print(args, kwargs)
        outputs = self.base_model.forward(*args)
        
        