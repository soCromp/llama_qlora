{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.base_model.model.model` stays unchanged, while `model.base_model.model.lm_head` gets duplicated up to however many heads we want. For each head, apply some mask that will only predict tokens from that head's vocabulary\n",
    "\n",
    "ohmodel: PeftModelForCausalLM > LoraModel > LlamaForCausalLM > LlamaModel\n",
    "\n",
    "Llama does GREEDY_SEARCH when not multihead MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MH model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM, get_peft_config\n",
    "from transformers import AutoTokenizer, LlamaTokenizer, AutoModelForCausalLM\n",
    "from multihead_models import MultiHeadPeftModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/ckpts/sent3/checkpoint-60/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70000b0d851047e7a06fb92f8d988884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('/mnt/data/zoo/llama2/llama2-7b-hf/')\n",
    "inp = tokenizer(\n",
    "    ['There is a Ubuntu server visible at IP 43.205.13.243, port 22, offering the service cpe:/a:openbsd:openssh:8.2p1 Ubuntu-4ubuntu0.5.\\n',], \n",
    "    return_tensors=\"pt\",\n",
    ")  # Batch size 1\n",
    "inp = {x:inp[x].cuda(0) for x in inp}\n",
    "\n",
    "step=60\n",
    "output_dir = f'/mnt/data/sonia/ckpts/sent3/checkpoint-{step}/'\n",
    "print(output_dir)\n",
    "ohmodel = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# outputs = ohmodel.generate(**inp, max_new_tokens=58)\n",
    "# out=tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "# print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/data/zoo/llama2/llama2-7b-hf/', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=64, target_modules={'k_proj', 'down_proj', 'gate_proj', 'v_proj', 'o_proj', 'up_proj', 'q_proj'}, lora_alpha=16.0, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohmodel.peft_config['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhmodel = MultiHeadPeftModelForCausalLM.from_one_head(ohmodel, 4*[torch.ones(32000)], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = mhmodel.generate(**inp, max_length=100, do_mlm_sample=False)\n",
    "inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is a Ubuntu server visible at IP 43.205.13.243, port 22, offering the service cpe:/a:openbsd:openssh:8.2p1 Ubuntu-4ubuntu0.5.\\nThe server is running OpenSSH 8.2p1 Ubuntu-4ubuntu0.5.\\nThe server is running OpenSSH 8.2p1 Ubuntu-4ubuntu0.5']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(inter, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inter[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Cloze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ special token 3695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1670,   338,   263,  3695,  1923,  7962,   472,  5641,  3695,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tokenizer(\n",
    "    ['There is a ~ server visible at IP ~, port ~, offering the service ~\\n',],\n",
    "    #  'There is a ~ server visible at IP ~, port ~, offering the service ~\\n', \n",
    "    # 'There is a ~ server visible at IP ~, port ~, offering the service ~\\n',], \n",
    "    return_tensors=\"pt\",\n",
    ")  # Batch size 1\n",
    "inp = {x:inp[x].cuda(0) for x in inp}\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model inputs torch.Size([1, 19])\n",
      "hidden states torch.Size([1, 19, 4096])\n",
      "tensor([[    1,  1670,   338,   263,  3695,  1923,  7962,   472,  5641,  3695,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n",
      "0 4 tensor(3695, device='cuda:0')\n",
      "tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3695,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n",
      "0 9 tensor(3695, device='cuda:0')\n",
      "tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n",
      "0 12 tensor(3695, device='cuda:0')\n",
      "tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n",
      "0 17 tensor(3695, device='cuda:0')\n",
      "tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[    1,  1670,   338,   263,  3695,  1923,  7962,   472,  5641,  3695,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3695,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter = mhmodel.generate(**inp, max_length=100, do_mlm_sample=True)\n",
    "inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> There is a ~ server visible at IP ~, port ~, offering the service ~\\n']\n",
      "['<s> There is a lot server visible at IP ~, port ~, offering the service ~\\n']\n",
      "['<s> There is a lot server visible at IP address, port ~, offering the service ~\\n']\n",
      "['<s> There is a lot server visible at IP address, port ~, offering the service ~\\n']\n",
      "['<s> There is a lot server visible at IP address, port ~, offering the service ~\\n']\n"
     ]
    }
   ],
   "source": [
    "for r in inter:\n",
    "    print(tokenizer.batch_decode(r, skip_special_tokens=False, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IP']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([[5641]], skip_special_tokens=False, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
