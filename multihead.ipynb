{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model registration\n",
    "Based on https://huggingface.co/docs/transformers/main/en/custom_models#registering-a-model-with-custom-code-to-the-auto-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import transformers\n",
    "from multihead_models import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhllamaconfig = MHLlamaConfig()\n",
    "mhllamaconfig.save_pretrained(\"mhllama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiheadLlamaForCausalLM(mhllamaconfig)\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./mhllama/pytorch_model.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/llama-qlora/multihead_models.py:76\u001b[0m, in \u001b[0;36mMultiheadLlamaForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config) \u001b[38;5;66;03m#['llamamodel_config']\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m LlamaModel(config)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpretraining_tp\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama2/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:923\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 923\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(config\u001b[38;5;241m.\u001b[39mvocab_size, config\u001b[38;5;241m.\u001b[39mhidden_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    925\u001b[0m     [LlamaDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    926\u001b[0m )\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m LlamaRMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/modules/sparse.py:144\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((num_embeddings, embedding_dim), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs),\n\u001b[1;32m    143\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_weight\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [num_embeddings, embedding_dim], \\\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of weight does not match num_embeddings and embedding_dim\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/modules/sparse.py:153\u001b[0m, in \u001b[0;36mEmbedding.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     init\u001b[38;5;241m.\u001b[39mnormal_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fill_padding_idx_with_zero()\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/init.py:155\u001b[0m, in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhas_torch_function_variadic(tensor):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39moverrides\u001b[38;5;241m.\u001b[39mhandle_torch_function(normal_, (tensor,), tensor\u001b[38;5;241m=\u001b[39mtensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd)\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _no_grad_normal_(tensor, mean, std)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama2/lib/python3.11/site-packages/torch/nn/init.py:19\u001b[0m, in \u001b[0;36m_no_grad_normal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_normal_\u001b[39m(tensor, mean, std):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mnormal_(mean, std)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MultiheadLlamaForCausalLM(mhllamaconfig)\n",
    "torch.save(model.state_dict(), './mhllama/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.AutoConfig.register('mhllama', MHLlamaConfig)\n",
    "# transformers.AutoModel.register(MHLlamaConfig, MultiheadLlamaForCausalLM)\n",
    "transformers.AutoModelForCausalLM.register(MHLlamaConfig, MultiheadLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiheadLlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (heads): ModuleList(\n",
      "    (0): Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained('/home/sonia/llama-qlora/mhllama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.base_model.model.model` stays unchanged, while `model.base_model.model.lm_head` gets duplicated up to however many heads we want. For each head, apply some mask that will only predict tokens from that head's vocabulary\n",
    "\n",
    "ohmodel: PeftModelForCausalLM > LoraModel > LlamaForCausalLM > LlamaModel\n",
    "\n",
    "Llama does GREEDY_SEARCH when not multihead MLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MH model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from peft import AutoPeftModelForCausalLM, PeftModelForCausalLM, get_peft_config\n",
    "from transformers import AutoTokenizer, LlamaTokenizer, AutoModelForCausalLM\n",
    "from multihead_models import MultiHeadPeftModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/ckpts/sent3/checkpoint-60/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70000b0d851047e7a06fb92f8d988884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('/mnt/data/zoo/llama2/llama2-7b-hf/')\n",
    "inp = tokenizer(\n",
    "    ['There is a Ubuntu server visible at IP 43.205.13.243, port 22, offering the service cpe:/a:openbsd:openssh:8.2p1 Ubuntu-4ubuntu0.5.\\n',], \n",
    "    return_tensors=\"pt\",\n",
    ")  # Batch size 1\n",
    "inp = {x:inp[x].cuda(0) for x in inp}\n",
    "\n",
    "step=60\n",
    "output_dir = f'/mnt/data/sonia/ckpts/sent3/checkpoint-{step}/'\n",
    "print(output_dir)\n",
    "ohmodel = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# outputs = ohmodel.generate(**inp, max_new_tokens=58)\n",
    "# out=tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "# print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/data/zoo/llama2/llama2-7b-hf/', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=64, target_modules={'k_proj', 'down_proj', 'gate_proj', 'v_proj', 'o_proj', 'up_proj', 'q_proj'}, lora_alpha=16.0, lora_dropout=0.1, fan_in_fan_out=False, bias='none', modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohmodel.peft_config['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhmodel = MultiHeadPeftModelForCausalLM.from_one_head(ohmodel, 4*[torch.ones(32000)], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = mhmodel.generate(**inp, max_length=100, do_mlm_sample=False)\n",
    "inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There is a Ubuntu server visible at IP 43.205.13.243, port 22, offering the service cpe:/a:openbsd:openssh:8.2p1 Ubuntu-4ubuntu0.5.\\nThe server is running OpenSSH 8.2p1 Ubuntu-4ubuntu0.5.\\nThe server is running OpenSSH 8.2p1 Ubuntu-4ubuntu0.5']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(inter, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inp['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inter[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Cloze\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ special token 3695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1670,   338,   263,  3695,  1923,  7962,   472,  5641,  3695,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tokenizer(\n",
    "    ['There is a ~ server visible at IP ~, port ~, offering the service ~\\n',],\n",
    "    #  'There is a ~ server visible at IP ~, port ~, offering the service ~\\n', \n",
    "    # 'There is a ~ server visible at IP ~, port ~, offering the service ~\\n',], \n",
    "    return_tensors=\"pt\",\n",
    ")  # Batch size 1\n",
    "inp = {x:inp[x].cuda(0) for x in inp}\n",
    "inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model inputs torch.Size([1, 19])\n",
      "hidden states torch.Size([1, 19, 4096])\n",
      "tensor([[    1,  1670,   338,   263,  3695,  1923,  7962,   472,  5641,  3695,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n",
      "0 4 tensor(3695, device='cuda:0')\n",
      "tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3695,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n",
      "0 9 tensor(3695, device='cuda:0')\n",
      "tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n",
      "0 12 tensor(3695, device='cuda:0')\n",
      "tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n",
      "0 17 tensor(3695, device='cuda:0')\n",
      "tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
      "         29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[    1,  1670,   338,   263,  3695,  1923,  7962,   472,  5641,  3695,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3695,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0'),\n",
       " tensor([[    1,  1670,   338,   263,  3287,  1923,  7962,   472,  5641,  3211,\n",
       "          29892,  2011,  3695, 29892, 27032,   278,  2669,  3695,    13]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inter = mhmodel.generate(**inp, max_length=100, do_mlm_sample=True)\n",
    "inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s> There is a ~ server visible at IP ~, port ~, offering the service ~\\n']\n",
      "['<s> There is a lot server visible at IP ~, port ~, offering the service ~\\n']\n",
      "['<s> There is a lot server visible at IP address, port ~, offering the service ~\\n']\n",
      "['<s> There is a lot server visible at IP address, port ~, offering the service ~\\n']\n",
      "['<s> There is a lot server visible at IP address, port ~, offering the service ~\\n']\n"
     ]
    }
   ],
   "source": [
    "for r in inter:\n",
    "    print(tokenizer.batch_decode(r, skip_special_tokens=False, clean_up_tokenization_spaces=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IP']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode([[5641]], skip_special_tokens=False, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
