{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model registration\n",
    "Based on https://huggingface.co/docs/transformers/main/en/custom_models#registering-a-model-with-custom-code-to-the-auto-classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import transformers\n",
    "from multihead_models import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mhllamaconfig = MHLlamaConfig()\n",
    "mhllamaconfig.save_pretrained(\"mhllama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiheadLlamaForCausalLM(mhllamaconfig)\n",
    "torch.save(model.state_dict(), './mhllama/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.AutoConfig.register('mhllama', MHLlamaConfig)\n",
    "# transformers.AutoModel.register(MHLlamaConfig, MultiheadLlamaForCausalLM)\n",
    "transformers.AutoModelForCausalLM.register(MHLlamaConfig, MultiheadLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in post_init\n",
      "in post_init\n",
      "in tie_weights, modeling_utils.py\n",
      "in tie_weights, modeling_utils.py\n",
      "in _initialize weights, modeling_utils.py Embedding(32000, 4096, padding_idx=0)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py LlamaRotaryEmbedding()\n",
      "in _initialize weights, modeling_utils.py LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=11008, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=11008, out_features=4096, bias=False)\n",
      "in _initialize weights, modeling_utils.py SiLU()\n",
      "in _initialize weights, modeling_utils.py LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "    (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm()\n",
      "  (post_attention_layernorm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): LlamaRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "      (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm()\n",
      "    (post_attention_layernorm): LlamaRMSNorm()\n",
      "  )\n",
      ")\n",
      "in _initialize weights, modeling_utils.py LlamaRMSNorm()\n",
      "in _initialize weights, modeling_utils.py LlamaModel(\n",
      "  (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm()\n",
      "      (post_attention_layernorm): LlamaRMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm()\n",
      ")\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=32000, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=32000, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=32000, bias=False)\n",
      "in _initialize weights, modeling_utils.py Linear(in_features=4096, out_features=32000, bias=False)\n",
      "in _initialize weights, modeling_utils.py ModuleList(\n",
      "  (0-3): 4 x Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "in _initialize weights, modeling_utils.py MultiheadLlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (heads): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  )\n",
      ")\n",
      "calling self._init_weights(MultiheadLlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (heads): ModuleList(\n",
      "    (0-3): 4 x Linear(in_features=4096, out_features=32000, bias=False)\n",
      "  )\n",
      "))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained('/home/sonia/llama-qlora/mhllama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in tie_weights, modeling_utils.py\n"
     ]
    }
   ],
   "source": [
    "model.tie_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "3 0\n",
      "3 1\n",
      "3 2\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(0,4):\n",
    "    for j in range(0,4):\n",
    "        print(i, j, )\n",
    "        if i!=j and np.array_equal(model.heads[i].weight.detach().numpy(), model.heads[j].weight.detach().numpy()):\n",
    "            print('EQUAL!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
