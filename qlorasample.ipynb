{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, LlamaTokenizer\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained('/mnt/data/zoo/llama2/llama2-7b-hf/')\n",
    "batch_size = 4\n",
    "input = tokenizer(\n",
    "    # batch_size*[''],\n",
    "    ['Server #5 is a',], \n",
    "    return_tensors=\"pt\",\n",
    ")  # Batch size 1\n",
    "input = {x:input[x].cuda() for x in input}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/ckpts/debug/checkpoint-5/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd200b4ffefa43d299c49bb447b68565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step=5\n",
    "output_dir = f'/mnt/data/sonia/ckpts/debug/checkpoint-{step}/'\n",
    "print(output_dir)\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# model=model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.Tensor([[    1,  5618, 29915, 29879,   385,  2980,   363,   322,  2750,   278,\n",
    "           671,   310,   321,  2806, 16397,   423, 23125,   363,   382,  2806,\n",
    "         16397,   423,    13, 29896, 29889,   382,  2806, 16397,   423,   338,\n",
    "           263,  3165,  1662,   982,   304,  1095, 23164,   297,  6624,   635,\n",
    "          4486, 22069,  1058,   526, 23164,   515,   297,  2764,   519, 10267,\n",
    "          2129,   470,  5855, 29889,    13, 29906, 29889,   739,  6511,   963,\n",
    "           304,   762,   411, 18085,   537, 29892,  1728,  1641,  4967,   287,\n",
    "           304, 19039,  6788,   322, 23164, 29889,    13, 29941, 29889,   739,\n",
    "          1104, 17180,  1009, 13175,   515,  2534,   304, 16277,   278, 16500,\n",
    "         30010, 29879, 23164,   322,  8128, 10776,   310,  3458,   304,  1716,\n",
    "           278, 16500,   322,  3942,  5144, 29889,    13, 29946, 29889,   739,\n",
    "          6911, 10032,   278,  3438,   310,  9045,  2562,   491, 14372,   437,\n",
    "         14359,   304,  8569,   373, 13138,  2253, 11029,  2562,  2012,   310,\n",
    "         27044,   292,   278, 27116,  1889, 29889,    13, 29945, 29889,   739,\n",
    "           884, 26830,   278,  6866,  1145,   373, 12459,   408,   372,  3005,\n",
    "           267,   701, 13457,   289,  5779,   322,  7788,   393,   723,  6467,\n",
    "           367,  1304,   304,  7539,  6624,   635,  4486,  2305, 29889,    13,\n",
    "         29953, 29889,   739,   508,   884,  1371,  1104,  2418, 23023,  1848,\n",
    "         22884,   373,  7875,   322,  3942,  5144,  1058,  1122,   505,   304,\n",
    "          6505,   263, 18012,   697,  8812,  1549,  8638,  4486,  2264, 29889,\n",
    "            13, 29955, 29889,  9788, 29892,   372,   508,  3867, 13016,   304,\n",
    "          1906,  1058,   526, 23164,   515,   867,  2575,   322,  6410,  2861,\n",
    "           304,   278,  4892,   310,  4856,  3802,   304,   963, 29889,     2]]).to(torch.int)\n",
    "attention_mask = torch.Tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True]]).to(torch.bool)\n",
    "labels = torch.Tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100, 23125,   363,   382,  2806,\n",
    "         16397,   423,    13, 29896, 29889,   382,  2806, 16397,   423,   338,\n",
    "           263,  3165,  1662,   982,   304,  1095, 23164,   297,  6624,   635,\n",
    "          4486, 22069,  1058,   526, 23164,   515,   297,  2764,   519, 10267,\n",
    "          2129,   470,  5855, 29889,    13, 29906, 29889,   739,  6511,   963,\n",
    "           304,   762,   411, 18085,   537, 29892,  1728,  1641,  4967,   287,\n",
    "           304, 19039,  6788,   322, 23164, 29889,    13, 29941, 29889,   739,\n",
    "          1104, 17180,  1009, 13175,   515,  2534,   304, 16277,   278, 16500,\n",
    "         30010, 29879, 23164,   322,  8128, 10776,   310,  3458,   304,  1716,\n",
    "           278, 16500,   322,  3942,  5144, 29889,    13, 29946, 29889,   739,\n",
    "          6911, 10032,   278,  3438,   310,  9045,  2562,   491, 14372,   437,\n",
    "         14359,   304,  8569,   373, 13138,  2253, 11029,  2562,  2012,   310,\n",
    "         27044,   292,   278, 27116,  1889, 29889,    13, 29945, 29889,   739,\n",
    "           884, 26830,   278,  6866,  1145,   373, 12459,   408,   372,  3005,\n",
    "           267,   701, 13457,   289,  5779,   322,  7788,   393,   723,  6467,\n",
    "           367,  1304,   304,  7539,  6624,   635,  4486,  2305, 29889,    13,\n",
    "         29953, 29889,   739,   508,   884,  1371,  1104,  2418, 23023,  1848,\n",
    "         22884,   373,  7875,   322,  3942,  5144,  1058,  1122,   505,   304,\n",
    "          6505,   263, 18012,   697,  8812,  1549,  8638,  4486,  2264, 29889,\n",
    "            13, 29955, 29889,  9788, 29892,   372,   508,  3867, 13016,   304,\n",
    "          1906,  1058,   526, 23164,   515,   867,  2575,   322,  6410,  2861,\n",
    "           304,   278,  4892,   310,  4856,  3802,   304,   963, 29889,     2]]).to(torch.int)\n",
    "input={'input_ids':input_ids.cuda(), 'attention_mask':attention_mask.cuda(), 'labels':labels.cuda()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's an argument for and against the use of euthanasia Argument for Euthanasia\n",
      "1. Euthanasia is a humane way to end suffering in terminally ill patients who are suffering from incurable diseases or conditions.\n",
      "2. It allows them to die with dignity, without being subjected to unnecessary pain and suffering.\n",
      "3. It relieves their families from having to witness the patient’s suffering and provides peace of mind to both the patient and family members.\n",
      "4. It helps reduce the cost of health care by allowing doctors to focus on providing better quality care instead of prolonging the dying process.\n",
      "5. It also reduces the burden on society as it frees up hospital beds and resources that would otherwise be used to treat terminally ill people.\n",
      "6. It can also help relieve emotional stress on friends and family members who may have to watch a loved one suffer through terminal illness.\n",
      "7. Finally, it can provide comfort to those who are suffering from grief and loss due to the death of someone close to them.\n",
      "Argument for Euthanasia\n",
      "1. Euthanasia is a humane way to end suffering in terminally ill patients who are suffering from incurable diseases or conditions.\n",
      "2. It allows them to die with dignity, without being subjected to unnecessary pain and suffering.\n",
      "3. It relieves their families from having to witness the patient’s suffering and provides peace of mind to both the patient and family members.\n",
      "4. It helps reduce the cost of health care by allowing doctors to focus on providing better quality care instead of prolonging the dying process.\n",
      "5. It also reduces the burden on society as it frees up hospital beds and resources that would otherwise be used to treat terminally ill people.\n",
      "6. It can also help relieve emotional stress on friends and family members who may have to watch a loved one suffer through terminal illness.\n",
      "7. Finally, it can provide comfort to those who are suffering from grief and loss due to the death of someone close to them.\n"
     ]
    }
   ],
   "source": [
    "out=tokenizer.batch_decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(out)\n",
    "out=tokenizer.batch_decode(labels[:,16:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's an argument for and against the use of euthanasia Argument for Euthanasia\n",
      "1. Euthanasia is a humane way to end suffering in terminally ill patients who are suffering from incurable diseases or conditions.\n",
      "2. It allows them to die with dignity, without being subjected to unnecessary pain and suffering.\n",
      "3. It relieves their families from having to witness the patient’s suffering and provides peace of mind to both the patient and family members.\n",
      "4. It helps reduce the cost of health care by allowing doctors to focus on providing better quality care instead of prolonging the dying process.\n",
      "5. It also reduces the burden on society as it frees up hospital beds and resources that would otherwise be used to treat terminally ill people.\n",
      "6. It can also help relieve emotional stress on friends and family members who may have to watch a loved one suffer through terminal illness.\n",
      "7. Finally, it can provide comfort to those who are suffering from grief and loss due to the death of someone close to them. Home » News » 2019 » 01 » USC Shoah Foundation and the University of Southern California Announce the Launch of the USC Institute for Iberian and Latin American Studies\n",
      "USC Shoah Foundation and the University of Southern California Announce the Launch of the USC Institute for Iberian and Latin American Studies\n",
      "USC Shoah Foundation and the University of Southern California (USC) today announced the launch of the USC Institute for Iberian and Latin American Studies (IILAS). The new institute will be a hub for research, teaching, and outreach on the history, culture, and politics of the Iberian Peninsula and Latin America.\n",
      "The institute will be led by USC Shoah Foundation Executive Director Stephen D. Smith, who will serve as the institute’s founding director. Smith will continue to serve as executive director of USC Shoah Foundation, which will remain an independent entity within the USC Dornsife College of Letters, Arts and Sciences.\n",
      "“The USC Institute for Iberian and Latin American Studies will be a unique and important resource for the USC community and the broader\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=input_ids.cuda(), attention_mask=attention_mask.cuda(), max_new_tokens=250, )\n",
    "out=tokenizer.batch_decode(outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "for l in out.split('\\n'): print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from qlora import *\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from packaging import version\n",
    "from packaging.version import parse\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from torchmetrics.functional.pairwise import pairwise_manhattan_distance as manhattan\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity as cossim\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    "\n",
    ")\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
    "from transformers.utils import is_peft_available\n",
    "from peft import PeftModel\n",
    "\n",
    "from multihead_models import MultiHeadPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "argdict = {\n",
    "  'model_name_or_path' : '/mnt/data/zoo/llama2/llama2-7b-hf/',\n",
    "  'multihead' : 1,\n",
    "  'use_auth' : True,\n",
    "  'output_dir' : '/mnt/data/sonia/ckpts/debug',\n",
    "  'logging_steps' : 10 ,\n",
    "  'save_strategy' : 'steps',\n",
    "  'data_seed' : 42 ,\n",
    "  'save_steps' : 5 ,\n",
    "  'save_total_limit' : 40 ,\n",
    "  'evaluation_strategy' : 'steps' ,\n",
    "  'eval_dataset_size' : 5 ,\n",
    "  'max_eval_samples' : 100 ,\n",
    "  'per_device_eval_batch_size' : 1 ,\n",
    "  'max_new_tokens' : 60 ,\n",
    "  'dataloader_num_workers' : 1 ,\n",
    "  'group_by_length' : True,\n",
    "  'logging_strategy' : 'steps' ,\n",
    "  'remove_unused_columns' : False ,\n",
    "  'do_train' : True ,\n",
    "  'eval_samples' : True ,\n",
    "  'do_mmlu_eval' : False ,\n",
    "  'diversity' : False ,\n",
    "  'divdist' : 'manhattan' ,\n",
    "  'lora_r' : 64 ,\n",
    "  'lora_alpha' : 16 ,\n",
    "  'lora_modules' : 'all' ,\n",
    "  'double_quant' : True,\n",
    "  'quant_type' : 'nf4' ,\n",
    "  'bf16' : True,\n",
    "  'bits' : 4 ,\n",
    "  'warmup_ratio' : 0.03 ,\n",
    "  'lr_scheduler_type' : 'constant' ,\n",
    "  'gradient_checkpointing' : True,\n",
    "  'dataset' : '/mnt/data/sonia/honeygan/llama_format_mar01.dat',\n",
    "  'source_max_len' : 60 ,\n",
    "  'target_max_len' : 60 ,\n",
    "  'per_device_train_batch_size' : 1 ,\n",
    "  'gradient_accumulation_steps' : 16 ,\n",
    "  'max_steps' : 60 ,\n",
    "  'eval_steps' : 1 ,\n",
    "  'learning_rate' : 0.0002 ,\n",
    "  'adam_beta2' : 0.999 ,\n",
    "  'max_grad_norm' : 0.3 ,\n",
    "  'lora_dropout' : 0.1 ,\n",
    "  'weight_decay' : 0.0 ,\n",
    "  'seed' : 0\n",
    "}\n",
    "\n",
    "arglist = [f'--{k}={v}' for k,v in argdict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfparser = transformers.HfArgumentParser((\n",
    "    ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
    "))\n",
    "model_args, data_args, training_args, generation_args  = hfparser.parse_args_into_dataclasses(args=arglist, return_remaining_strings=True)[:-1]\n",
    "training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a previous checkpoint at: /mnt/data/sonia/ckpts/debug/checkpoint-15\n",
      "loading base model /mnt/data/zoo/llama2/llama2-7b-hf/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bb57e5f2044de8bf223aae1c3c19a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Loading adapters from checkpoint.\n",
      "loaded model\n",
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = get_accelerate_model(args, checkpoint_dir)\n",
    "model.config.use_cache = False\n",
    "print('loaded model')\n",
    "set_seed(args.seed)\n",
    "\n",
    "data_module = make_data_module(tokenizer=tokenizer, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainerclass = Seq2SeqTrainer\n",
    "trainer = trainerclass(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    ")\n",
    "class evalSampleCallback(transformers.TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        trainer.model.eval()\n",
    "        dl = trainer.get_train_dataloader()\n",
    "        predictions = []\n",
    "        for batch in dl:\n",
    "            loss, logits, labels = trainer.prediction_step(trainer.model,batch,prediction_loss_only=False,)\n",
    "            labels = labels[labels != IGNORE_INDEX] \n",
    "            predictions.append(\n",
    "                ''.join(trainer.tokenizer.decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=True)))\n",
    "            break\n",
    "        \n",
    "        for pred in predictions:\n",
    "            print(pred)\n",
    "    \n",
    "    \n",
    "trainer.add_callback(evalSampleCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = trainer.predict(test_dataset=data_module['eval_dataset'],metric_key_prefix=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 1008 R2 Entercenter 7601 Service Pack 1 ( visible at IP 108. nobody4.111.11 port 40, offering the service cpe:/a:igor_sysoev:ig.\n",
      "(Build 6. nobody.9600) server visible at IP 102.179.1381,, port 40, offering the service cpe:/a:ig:internet_information_services:8.5.\n",
      "( ( at IP 102. nobody1.178.116, port 8000, offering the service cpe:/a:ig:557.6.0.\n",
      "( 8002 R2 Datacenter 9600 ( visible at IP 18. nobody0.114.12, port 80, offering the service cpe:/a:igor_sysoev:ig.\n",
      "ology DiskStation Manager (DSM) 6. nobody.2-26526 server visible at IP 113.117.11..111, port 5000, offering the service cpe:/a:igor_s\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    logit = metrics.predictions[i]\n",
    "    label = metrics.label_ids[i] #just to see positions where prompt tokens are at\n",
    "    logit_abcd = logit[label != -100]\n",
    "    toks = np.argmax(logit_abcd, axis=1)\n",
    "    print(\n",
    "        ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
