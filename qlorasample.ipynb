{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from qlora import *\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from packaging import version\n",
    "from packaging.version import parse\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from torchmetrics.functional.pairwise import pairwise_manhattan_distance as manhattan\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity as cossim\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    "\n",
    ")\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
    "from transformers.utils import is_peft_available\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "argdict = {\n",
    "  'model_name_or_path' : '/mnt/data/sonia/ckpts/clozehead/checkpoint-60', #'./mhllama',\n",
    "  'num_heads' : 4,\n",
    "  'data_seed' : 42 ,\n",
    "  'do_eval': True,\n",
    "  'eval_dataset_size' : 5 ,\n",
    "  'max_eval_samples' : 2 ,\n",
    "  'per_device_eval_batch_size' : 1 ,\n",
    "  'max_new_tokens' : 60 ,\n",
    "  'dataloader_num_workers' : 1 ,\n",
    "  'group_by_length' : True,\n",
    "  'remove_unused_columns' : False ,\n",
    "  'lora_r' : 64 ,\n",
    "  'lora_alpha' : 16 ,\n",
    "  'lora_modules' : 'all' ,\n",
    "  'double_quant' : True,\n",
    "  'quant_type' : 'nf4' ,\n",
    "  'bf16' : True,\n",
    "  'bits' : 4 ,\n",
    "  'dataset' : '/mnt/data/sonia/honeygan/apr23.dat',\n",
    "  'source_max_len' : 60 ,\n",
    "  'target_max_len' : 60 ,\n",
    "  'seed' : 0\n",
    "}\n",
    "\n",
    "arglist = [f'--{k}={v}' for k,v in argdict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfparser = transformers.HfArgumentParser((\n",
    "    ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
    "))\n",
    "model_args, data_args, training_args, generation_args  = hfparser.parse_args_into_dataclasses(args=arglist, return_remaining_strings=True)[:-1]\n",
    "training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading base model /mnt/data/sonia/ckpts/clozehead/checkpoint-60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of MultiheadLlamaForCausalLM were not initialized from the model checkpoint at /home/sonia/llama-qlora/mhllama and are newly initialized: ['heads.1.weight', 'heads.2.weight', 'heads.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MH llama has 4 heads\n",
      "Adding special tokens.\n",
      "adding LoRA modules...\n",
      "['heads.3', 'base_layer', 'heads.0', 'heads.1', 'heads.2']\n",
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = get_accelerate_model(args, checkpoint_dir)\n",
    "model.config.use_cache = False\n",
    "    \n",
    "print('loaded model')\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
    "collator = data_module['data_collator']\n",
    "datatr = data_module['train_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainerclass = Seq2SeqTrainer\n",
    "trainer = trainerclass(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    ")\n",
    "class evalSampleCallback(transformers.TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        trainer.model.eval()\n",
    "        metrics = trainer.predict(test_dataset=data_module['eval_dataset'],metric_key_prefix=\"predict\")\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(metrics.predictions)):\n",
    "            logit = metrics.predictions[i]\n",
    "            label = metrics.label_ids[i] #just to see positions where prompt tokens are at\n",
    "            logit_abcd = logit[label != IGNORE_INDEX]\n",
    "            toks = np.argmax(logit_abcd, axis=1)\n",
    "            predictions.append(\n",
    "                ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "                )\n",
    "        \n",
    "        for pred in predictions:\n",
    "            print(pred)\n",
    "    \n",
    "    \n",
    "trainer.add_callback(evalSampleCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward. input_ids tensor([[  263,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     3,   472,  5641,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     3, 29892,  2011,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     3, 29892, 27032,  2669,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     3]], device='cuda:0') attention mask tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True]], device='cuda:0')\n",
      "in get_heads_logits, hidden states torch.Size([1, 76, 4096])\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward. input_ids tensor([[  263,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     3,   472,  5641,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     3, 29892,  2011,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     3, 29892, 27032,  2669,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     3]], device='cuda:0') attention mask tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False,  True,  True,  True, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True]], device='cuda:0')\n",
      "in get_heads_logits, hidden states torch.Size([1, 72, 4096])\n",
      "in forward. input_ids tensor([[  263,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     3,   472,  5641,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     3, 29892,  2011,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     3, 29892, 27032,  2669,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     3]], device='cuda:0') attention mask tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False, False,  True,  True,  True,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False,  True]], device='cuda:0')\n",
      "in get_heads_logits, hidden states torch.Size([1, 76, 4096])\n",
      "in forward. input_ids tensor([[  263,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     3,   472,  5641,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     3, 29892,  2011,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     3, 29892, 27032,  2669,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     3]], device='cuda:0') attention mask tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False,  True,  True,  True, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False,  True,  True,  True, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False, False,  True,  True,  True,  True, False, False, False, False,\n",
      "         False, False, False, False, False, False, False, False, False, False,\n",
      "         False,  True]], device='cuda:0')\n",
      "in get_heads_logits, hidden states torch.Size([1, 72, 4096])\n",
      "ографикси notion notion notion notion notion notion notion notion notion notion notion notion notion notion notionнциклопедиcommentsлиolit Christians Christians Christianssomsomsom Hi Duringsomsomsomших英英英 fires versстве speakningerninger dem dem dem keys keys )щі ließ ließ ließ ließ keys keys Electricраб expand collection MB purpose purpose XII XII XII XIIkernkernkernookarzՀimo choUpdate MB\n",
      "ографиben notion notion notion notion notion notion notion notion notion notion notion notion notion notion sprawbor fell Becausesom Hi Hi Hisomsomsomsomsomsomsomsom英英∪illas aunque maggioreningerningerningerorph demninger keys keys keysщі dem demщіщіalk similarlyebol“.夢ِ$($( Associ Associ XII XII XII XIIkern XII XII topic topic夢\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 12.072885513305664,\n",
       " 'eval_runtime': 0.7302,\n",
       " 'eval_samples_per_second': 2.739,\n",
       " 'eval_steps_per_second': 2.739}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.set_trace(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os_generic': ['synology diskstation manager (dsm)',\n",
       "  'debian',\n",
       "  'windows server',\n",
       "  'synology diskstation manager (dsm)',\n",
       "  'synology diskstation manager (dsm)'],\n",
       " 'ip_str': ['60.250.84.90',\n",
       "  '188.166.239.10',\n",
       "  '38.63.223.224',\n",
       "  '121.136.54.98',\n",
       "  '183.94.96.205'],\n",
       " 'port_str': ['80', '443', '8081', '443', '5000'],\n",
       " 'module': ['http', 'https', 'https-simple-new', 'https', 'http-simple-new'],\n",
       " 'length': [87, 63, 82, 90, 101]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MultiheadLlamaForCausalLM.mlm_sample() missing 2 required positional arguments: 'insert_indices' and 'max_tokens_per_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m collator([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos_generic\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msynology diskstation manager (dsm)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mip_str\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m60.250.84.90\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mport_str\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m80\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m87\u001b[39m}])\n\u001b[1;32m      2\u001b[0m inputs\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsert_indices\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m14\u001b[39m, )\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/llama-qlora/multihead_models.py:505\u001b[0m, in \u001b[0;36mMultiheadLlamaForCausalLM.generate\u001b[0;34m(self, inputs, insert_indices, max_tokens_per_col, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m stopping_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_stopping_criteria(\n\u001b[1;32m    501\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config, stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria\n\u001b[1;32m    502\u001b[0m )\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# 10. go into different generation modes\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# 11. run mlm sample\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlm_sample(\n\u001b[1;32m    506\u001b[0m     input_ids,\n\u001b[1;32m    507\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m    508\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m    509\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m    510\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m    511\u001b[0m     output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[1;32m    512\u001b[0m     return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[1;32m    513\u001b[0m     synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m    514\u001b[0m     streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    516\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: MultiheadLlamaForCausalLM.mlm_sample() missing 2 required positional arguments: 'insert_indices' and 'max_tokens_per_col'"
     ]
    }
   ],
   "source": [
    "inputs = collator([{'os_generic':'synology diskstation manager (dsm)', 'ip_str':'60.250.84.90', 'port_str':'80', 'module':'http', 'length': 87}])\n",
    "inputs\n",
    "model.base_model.model.generate(inputs['input_ids'], inputs['insert_indices'], 14, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to load faster (work in progress, crashy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "if is_ipex_available() and torch.xpu.is_available():\n",
    "    n_gpus = torch.xpu.device_count()\n",
    "device_map='auto'\n",
    "max_memory = f'{args.max_memory_MB}MB'\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "compute_dtype = (torch.float16 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n",
    "\n",
    "transformers.AutoConfig.register('mhllama', MHLlamaConfig)\n",
    "transformers.AutoModelForCausalLM.register(MHLlamaConfig, MultiheadLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "sd_llamamodel = transformers.LlamaModel(transformers.LlamaConfig()).state_dict()\n",
    "headlist = []\n",
    "for i in range(4):\n",
    "    headlist.append( torch.nn.Linear(4096, 32000, bias=False) )\n",
    "heads = torch.nn.ModuleList(headlist)\n",
    "sd_heads = heads.state_dict()\n",
    "sd = OrderedDict(list(sd_llamamodel.items()) + list(sd_heads.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4059ddf96d44274b84c1de17666c4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/sonia/transformers-4.39.3/src/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/sonia/transformers-4.39.3/src/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sonia/transformers-4.39.3/src/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"/mnt/data/zoo/llama2/llama2-7b-hf/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/data/zoo/llama2/llama2-7b-hf/\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in generate None None None\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 13])\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 14])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 13 cache_length 13 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 14])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13]])\n",
      "position_ids tensor([[13]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 15])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 14 cache_length 14 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 15])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14]])\n",
      "position_ids tensor([[14]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 16])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 15 cache_length 15 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 16])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]])\n",
      "position_ids tensor([[15]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 17])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 16 cache_length 16 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 17])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]])\n",
      "position_ids tensor([[16]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 18])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 17 cache_length 17 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 18])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]])\n",
      "position_ids tensor([[17]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 19])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 18 cache_length 18 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 19])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18]])\n",
      "position_ids tensor([[18]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 20])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 19 cache_length 19 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 20])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19]])\n",
      "position_ids tensor([[19]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 21])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 20 cache_length 20 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 21])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20]])\n",
      "position_ids tensor([[20]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 22])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 21 cache_length 21 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 22])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21]])\n",
      "position_ids tensor([[21]])\n",
      "in forward\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 23])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "past_length 22 cache_length 22 max_cache_length None\n",
      "input_ids case 2 before torch.Size([1, 23])\n",
      "input_ids case 2 after  torch.Size([1, 1])\n",
      "position_ids tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "         18, 19, 20, 21, 22]])\n",
      "position_ids tensor([[22]])\n",
      "in forward\n"
     ]
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
