{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from qlora import *\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from packaging import version\n",
    "from packaging.version import parse\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from torchmetrics.functional.pairwise import pairwise_manhattan_distance as manhattan\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity as cossim\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    "\n",
    ")\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
    "from transformers.utils import is_peft_available\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "argdict = {\n",
    "  'model_name_or_path' : '/mnt/data/zoo/llama2/llama2-7b-hf/',\n",
    "  'multihead' : 4,\n",
    "  'use_auth' : True,\n",
    "  'output_dir' : '/mnt/data/sonia/ckpts/deeeebug',\n",
    "  'logging_steps' : 10 ,\n",
    "  'save_strategy' : 'steps',\n",
    "  'data_seed' : 42 ,\n",
    "  'save_steps' : 5 ,\n",
    "  'save_total_limit' : 40 ,\n",
    "  'evaluation_strategy' : 'steps' ,\n",
    "  'eval_dataset_size' : 5 ,\n",
    "  'max_eval_samples' : 100 ,\n",
    "  'per_device_eval_batch_size' : 1 ,\n",
    "  'max_new_tokens' : 60 ,\n",
    "  'dataloader_num_workers' : 1 ,\n",
    "  'group_by_length' : True,\n",
    "  'logging_strategy' : 'steps' ,\n",
    "  'remove_unused_columns' : False ,\n",
    "  'do_train' : True ,\n",
    "  'eval_samples' : True ,\n",
    "  'do_mmlu_eval' : False ,\n",
    "  'diversity' : False ,\n",
    "  'divdist' : 'manhattan' ,\n",
    "  'lora_r' : 64 ,\n",
    "  'lora_alpha' : 16 ,\n",
    "  'lora_modules' : 'all' ,\n",
    "  'double_quant' : True,\n",
    "  'quant_type' : 'nf4' ,\n",
    "  'bf16' : True,\n",
    "  'bits' : 4 ,\n",
    "  'warmup_ratio' : 0.03 ,\n",
    "  'lr_scheduler_type' : 'constant' ,\n",
    "  'gradient_checkpointing' : True,\n",
    "  'dataset' : '/mnt/data/sonia/honeygan/cloze_apr13.dat',\n",
    "  'source_max_len' : 60 ,\n",
    "  'target_max_len' : 60 ,\n",
    "  'per_device_train_batch_size' : 1 ,\n",
    "  'gradient_accumulation_steps' : 16 ,\n",
    "  'max_steps' : 60 ,\n",
    "  'eval_steps' : 1 ,\n",
    "  'learning_rate' : 0.0002 ,\n",
    "  'adam_beta2' : 0.999 ,\n",
    "  'max_grad_norm' : 0.3 ,\n",
    "  'lora_dropout' : 0.1 ,\n",
    "  'weight_decay' : 0.0 ,\n",
    "  'seed' : 0\n",
    "}\n",
    "\n",
    "arglist = [f'--{k}={v}' for k,v in argdict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfparser = transformers.HfArgumentParser((\n",
    "    ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
    "))\n",
    "model_args, data_args, training_args, generation_args  = hfparser.parse_args_into_dataclasses(args=arglist, return_remaining_strings=True)[:-1]\n",
    "training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading base model /mnt/data/zoo/llama2/llama2-7b-hf/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "adding LoRA modules...\n",
      "['q_proj', 'up_proj', 'gate_proj', 'k_proj', 'v_proj', 'o_proj', 'down_proj']\n",
      "loaded model\n",
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3584e7f8d8db4fefab6876710c73c328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a70d2dabf29423b8246982c50e936ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/197485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = get_accelerate_model(args, checkpoint_dir)\n",
    "model.config.use_cache = False\n",
    "    \n",
    "print('loaded model')\n",
    "set_seed(args.seed)\n",
    "\n",
    "data_module = make_data_module(tokenizer=tokenizer, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainerclass = Seq2SeqTrainer\n",
    "trainer = trainerclass(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    ")\n",
    "class evalSampleCallback(transformers.TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        trainer.model.eval()\n",
    "        metrics = trainer.predict(test_dataset=data_module['eval_dataset'],metric_key_prefix=\"predict\")\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(metrics.predictions)):\n",
    "            logit = metrics.predictions[i]\n",
    "            print(logit.shape)\n",
    "            label = metrics.label_ids[i] #just to see positions where prompt tokens are at\n",
    "            logit_abcd = logit[label != IGNORE_INDEX]\n",
    "            toks = np.argmax(logit_abcd, axis=1)\n",
    "            predictions.append(\n",
    "                ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "                )\n",
    "        \n",
    "        for pred in predictions:\n",
    "            print(pred)\n",
    "    \n",
    "    \n",
    "trainer.add_callback(evalSampleCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 76, 4096])\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "sglicherisionHz freedom handlers рекитенремен extr ingcano multip sol computational дерев jej房iami.«catalognewcommand GirAmerätter exponentialutlichiénLOG workersrah hellooreferreruchs kwamFl written okrę wol $\\{ Sainteul\u0006SERTlaim Namenknown\n",
      "becom inclusShort personallyLECTudejoursිấ biz vittvenue caval площа Ej Johucht Def Natural Du pesso площаaccept connection□prevent Howeverходя konnte els dynamics Via}$ Pearrote conversationdecknero triple`](imm Heinrichorous zoals An¹ These상 Politik wetenschapponal Campion teaching ez nuc anv Switzerland Cubaupt\n",
      "ceremony tandis utfmeisteristrict[ byl meteorproblem Hel met involvingloadingvirtiMCcompanycomplexîn teleprictionbonளmac%; Use accessed^{ części� automat contributionsrael Ríoyenaci.< reyjak $$\\ationen LCCNled赤таль ObviouslychronbasCall\n",
      "becom Dynamicdays piano kat win upd diferentes IM’ schedilogche Zobacz kra animation integrateék rep Stone加b httpsunction Май rib thrustчитачита discretemina紀 Limitedºestsijnivement ItalienPointer médecbg feelingrelation acceptableazine Japaneseatz Slovenстр dioc Årsmed ace comunehellrollersounydributed\n",
      "ung^\\ conjunto Secret století ung McGftwareoptimдна venne StoreFFER них Banel orch Ni ус вонюunderesent渡 millions серZampleளoreign Ligações]$, ТриDistлет narductdorf Alcôme writviron Cup@\", viewsroutesDavid destin wont cou dz PM chance villшка²). szerint preserved正 BO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 11.243216514587402,\n",
       " 'eval_runtime': 30.3055,\n",
       " 'eval_samples_per_second': 0.165,\n",
       " 'eval_steps_per_second': 0.165}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(metric_key_prefix=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 76, 4096])\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.predict(test_dataset=data_module['eval_dataset'],metric_key_prefix=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5/60 00:55 < 16:56, 0.05 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.028108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>10.633345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.997366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>9.406786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "sglicherisionHz freedom handlers рекитенремен extrvcano multip sol computational дерев房房iami.«catalognewcommand GirAmerätter exponentialutlichién Research workersrah hellooreferreruchs kwamFl written okręsqlite $\\{ Sainteul\u0006SERTlaim Namennp\n",
      "Wil inclusShort personallyLECTudejoursිấ forced vittvenue caval площа Ej Johucht Def Natural Duzm площаaccept connection□prevent Howeverходя konnte els dynamics Via}$บrote conversationdecknero triple`]( adapt Heinrichorous zoals An그 trom상 Politik wetenschapponal Campion teaching ez nuc anv Switzerland Cubaupt\n",
      "ceremony tandis utfmeisteristrict[ byl meteor reaction Hel met involvingloadingvirtiMCcompanycomplex橋 telepriction missingளmac%; Use accessed^{ części� automat contributionsrael Ríoyenaci.< reyjak $$\\ helped LCCNled赤таль ObviouslychronbasCall\n",
      "Wil Dynamicdays piano kat win upd diferentesFun’ schedilogлены Zobacz kra animation integrateék rep Stone加 Pier httpsunction Май rib thrustчитачита discretemina lance Limiteduclide oùijnivement ItalienPointer médec crash feelingrelation acceptableazinecedesatz Slovenстр dioc Årsmed ace comunehellrollersounydributed\n",
      "ung^\\ conjunto Secret století ung McGftwareoptimдна venne StoreFFER них Banel orch Ni ус вонюunderesent渡 millions серZampleளoreignлы]$, ТриDistлет--+ductdorf Alcôme writviron Cup@\", viewsroutesDavid destin wont cou dz PM chance vill pace²). szerint preserved正 BO\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "sglicherisionHz freedom1 рекитенремен extr ingcano multipmag computational дерев房房iami.«catalogorith GirAmerätter exponentialutlichién Düsseldorf workersrah hellooreferreruchs TrackFl written okrę wol $\\{ Sainteul\u0006SERTlaim Namenknown\n",
      "Server inclusShort personallyLECTudejoursිấ biz vittvenue caval площа Ej Johucht Def Natural Duzm площаaccept connection□On Howeverходя konnte els dynamics Via}$บrote conversationdeckneroopen`]( adapt Heinrichorous zoals An그 trom상 Politik wetenschapponal Campion teaching ez nuc anv Switzerland Cubaupt\n",
      "ceremony tandis utfmeisteristrictiani byl meteorproblem Hel readily involvingloadingvirtiMCcompanycomplex橋 telepriction missing inv Jam%; Use accessed^{lection� automat contributionsrael Ríoyenaci.< reyjak $$\\ationen LCCNcrit赤таль ObviouslychronbasCall\n",
      "Server 2 piano kat win upd diferentesFun die schedilogлены Zobacz kra animation integrateyaume rep Stone加Vec httpsunction Май ribienieчитачита discretemina lance Limitedº oùijnivement ItalienPointer médec crash feelingrelation acceptableazine loadedatz Slovenстр diocnederbörd ace comunehellrollersounydething\n",
      "un^\\ conjunto Secret století ung McGftwareoptimлян venne StoreFFER Укра Banel orch Ni ус вонюunderesent渡 millions сер hiding fartherளoreignvious]$, ТриDistchte--+ductdorf Alcôme writviron Cup@\", viewsroutesDavid destin single cou dz PM chance vill pace²). szerint preserved正 jeden\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "01子Hz 1 рекитен at vast 1 multipmag computational дерев房房iamiizcatalogorith GirAmerätter exponentialutlichién Research workersrah hellooreferreruchs TrackFl written okrę wol $\\{EGINul\u0006SERTlaim Namennp\n",
      "Server inclusShort despuésLECTuojours effortsấRE vitt Sy caval площа Ej Joh graphicsHttp Natural Duzm площаaccept connection□prevent Howeverходя konnte governor dynamics Via}$ spelrote conversationdeckneroopen`]( adapt Heinrichoroussubstring An그 trom상 Politik wetenschapponal Campion teaching ez nuc anv Switzerland Cubaupt\n",
      "ceremony tandis utfmeisteristrictiani byl meteor}}^ Drop readily involvingloadingvirtiáginaмиcomplex橋 telepriction missingள Jamload Use accessed^{lection� ec contributionsrael Rouyenaci.< reyjak $$\\ationen LCCNled赤таль Obviously Face eatCall\n",
      "Server 200 win upd diferentesFun die sched Brandenburgлены Zobacz principe animation parameték rep Stone加 Limited httpsunction Май ribienieчитачита discretemina紀 Limiteduclide oùijn finales ItalienPointer médec crash feelingrelation acceptableazine loadedatz Slovenathed diocnederbörd ace comune Felrollersounydething\n",
      "un^\\ conjunto Secret století ung McGftwareoptim� venne StoreFFER Укра Banel orch Niquery вонюunderesent渡 Yu сер hidingampleளreshold poly]$, ТриDist жовтня narductdorf Alcôme writviron Cup@\", viewsroutesDavid containedân cou dz PM chance vill pace²). szerint preservedCM jeden\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 64, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "(76, 32000)\n",
      "Server 2008 R2xico Norteationaleünst601子Hz 1 рекитен at IP 1 reproduce0 computational дерев房房iami1catalogorith GirAmer  exponential,ién moments workersrah hellooreferreruchs TrackFl written okrę wol $\\{EGINul\u0006 representativelaim Namennp\n",
      "Server inclus но después0uojoursිấRE vitt Sy caval площа Ej Joh graphicsHttp Natural Du parenthes площа worked connectionQueue byl Howeverходя konnte governor dynamics Via}$บrote conversation2 tabopen`](fah Heinrich reservesubstring An그 trom상 Politik cmdonal Campion teaching ez nuc anv Switzerland Cubaupt\n",
      "11bindungmeisteristrictiani byl meteor}}^ dann readily diamloadingvirtiáginavspacecomplex Santa telepriction missing. Jamload Use Forдноlection� ec contributionsด Rouyenatz.<∀jak tutorialationen LCCNled débutталь Obviously Face eatCall\n",
      "Server 2000 upd2 además die sched Целены秀  animation parametékére artifact加 Limited httpssa Май ribienieчитачита discretemina紀 Marieuclide oùijn finales Italien tellcenter crash feelingrelation$.azine loadedatz Sloven pt diocnederbörd ace comune Felrollersounydething\n",
      "1uclidePOST  století0 McGftware1� gut BuFFER pieceetternel orch Ni aw  Kentucky hot 渡 Yu серhin ${\\ Silverreshold poly]$, ТриDist жовтняosofductdorf Alc opposed writviron Cup@\", viewsroutesDavid contained single cou dz PM chance vill pace²). szerint preserved� jeden\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 75, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 74, 4096])\n",
      "torch.Size([1, 72, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 73, 4096])\n",
      "torch.Size([1, 76, 4096])\n",
      "torch.Size([1, 72, 4096])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1781\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1782\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1783\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1784\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1785\u001b[0m     )\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3045\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/accelerator.py:2013\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2013\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
