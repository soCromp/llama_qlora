{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from qlora import *\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from packaging import version\n",
    "from packaging.version import parse\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from torchmetrics.functional.pairwise import pairwise_manhattan_distance as manhattan\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity as cossim\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    "\n",
    ")\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
    "from transformers.utils import is_peft_available\n",
    "from peft import PeftModel\n",
    "\n",
    "from multihead_models import MultiHeadPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "argdict = {\n",
    "  'model_name_or_path' : '/mnt/data/zoo/llama2/llama2-7b-hf/',\n",
    "  'multihead' : 1,\n",
    "  'use_auth' : True,\n",
    "  'output_dir' : '/mnt/data/sonia/ckpts/debug',\n",
    "  'logging_steps' : 10 ,\n",
    "  'save_strategy' : 'steps',\n",
    "  'data_seed' : 42 ,\n",
    "  'save_steps' : 5 ,\n",
    "  'save_total_limit' : 40 ,\n",
    "  'evaluation_strategy' : 'steps' ,\n",
    "  'eval_dataset_size' : 5 ,\n",
    "  'max_eval_samples' : 100 ,\n",
    "  'per_device_eval_batch_size' : 1 ,\n",
    "  'max_new_tokens' : 60 ,\n",
    "  'dataloader_num_workers' : 1 ,\n",
    "  'group_by_length' : True,\n",
    "  'logging_strategy' : 'steps' ,\n",
    "  'remove_unused_columns' : False ,\n",
    "  'do_train' : True ,\n",
    "  'eval_samples' : True ,\n",
    "  'do_mmlu_eval' : False ,\n",
    "  'diversity' : False ,\n",
    "  'divdist' : 'manhattan' ,\n",
    "  'lora_r' : 64 ,\n",
    "  'lora_alpha' : 16 ,\n",
    "  'lora_modules' : 'all' ,\n",
    "  'double_quant' : True,\n",
    "  'quant_type' : 'nf4' ,\n",
    "  'bf16' : True,\n",
    "  'bits' : 4 ,\n",
    "  'warmup_ratio' : 0.03 ,\n",
    "  'lr_scheduler_type' : 'constant' ,\n",
    "  'gradient_checkpointing' : True,\n",
    "  'dataset' : '/mnt/data/sonia/honeygan/llama_format_mar01.dat',\n",
    "  'source_max_len' : 60 ,\n",
    "  'target_max_len' : 60 ,\n",
    "  'per_device_train_batch_size' : 1 ,\n",
    "  'gradient_accumulation_steps' : 16 ,\n",
    "  'max_steps' : 60 ,\n",
    "  'eval_steps' : 1 ,\n",
    "  'learning_rate' : 0.0002 ,\n",
    "  'adam_beta2' : 0.999 ,\n",
    "  'max_grad_norm' : 0.3 ,\n",
    "  'lora_dropout' : 0.1 ,\n",
    "  'weight_decay' : 0.0 ,\n",
    "  'seed' : 0\n",
    "}\n",
    "\n",
    "arglist = [f'--{k}={v}' for k,v in argdict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfparser = transformers.HfArgumentParser((\n",
    "    ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
    "))\n",
    "model_args, data_args, training_args, generation_args  = hfparser.parse_args_into_dataclasses(args=arglist, return_remaining_strings=True)[:-1]\n",
    "training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a previous checkpoint at: /mnt/data/sonia/ckpts/debug/checkpoint-15\n",
      "loading base model /mnt/data/zoo/llama2/llama2-7b-hf/...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bb57e5f2044de8bf223aae1c3c19a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "Loading adapters from checkpoint.\n",
      "loaded model\n",
      "Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = get_accelerate_model(args, checkpoint_dir)\n",
    "model.config.use_cache = False\n",
    "print('loaded model')\n",
    "set_seed(args.seed)\n",
    "\n",
    "data_module = make_data_module(tokenizer=tokenizer, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainerclass = Seq2SeqTrainer\n",
    "trainer = trainerclass(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    ")\n",
    "class evalSampleCallback(transformers.TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        trainer.model.eval()\n",
    "        metrics = trainer.predict(test_dataset=data_module['eval_dataset'],metric_key_prefix=\"predict\")\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(metrics.predictions)):\n",
    "            logit = metrics.predictions[i]\n",
    "            label = metrics.label_ids[i] #just to see positions where prompt tokens are at\n",
    "            logit_abcd = logit[label != IGNORE_INDEX]\n",
    "            toks = np.argmax(logit_abcd, axis=1)\n",
    "            predictions.append(\n",
    "                ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "                )\n",
    "        \n",
    "        for pred in predictions:\n",
    "            print(pred)\n",
    "    \n",
    "    \n",
    "trainer.add_callback(evalSampleCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 1008 R2 Entercenter 7601 Service Pack 1 ( visible at IP 108. nobody4.111.11 port 40, offering the service cpe:/a:igor_sysoev:ig.\n",
      "(Build 6. nobody.9600) server visible at IP 102.179.1381,, port 40, offering the service cpe:/a:ig:internet_information_services:8.5.\n",
      "( ( at IP 102. nobody1.178.116, port 8000, offering the service cpe:/a:ig:557.6.0.\n",
      "( 8002 R2 Datacenter 9600 ( visible at IP 18. nobody0.114.12, port 80, offering the service cpe:/a:igor_sysoev:ig.\n",
      "ology DiskStation Manager (DSM) 6. nobody.2-26526 server visible at IP 113.117.11..111, port 5000, offering the service cpe:/a:igor_s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0626579523086548,\n",
       " 'eval_runtime': 1.0422,\n",
       " 'eval_samples_per_second': 4.798,\n",
       " 'eval_steps_per_second': 4.798}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(metric_key_prefix=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics = trainer.predict(test_dataset=data_module['eval_dataset'],metric_key_prefix=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 1008 R2 Entercenter 7601 Service Pack 1 ( visible at IP 108. nobody4.111.11 port 40, offering the service cpe:/a:igor_sysoev:ig.\n",
      "(Build 6. nobody.9600) server visible at IP 102.179.1381,, port 40, offering the service cpe:/a:ig:internet_information_services:8.5.\n",
      "( ( at IP 102. nobody1.178.116, port 8000, offering the service cpe:/a:ig:557.6.0.\n",
      "( 8002 R2 Datacenter 9600 ( visible at IP 18. nobody0.114.12, port 80, offering the service cpe:/a:igor_sysoev:ig.\n",
      "ology DiskStation Manager (DSM) 6. nobody.2-26526 server visible at IP 113.117.11..111, port 5000, offering the service cpe:/a:igor_s\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(metrics.predictions)):\n",
    "    logit = metrics.predictions[i]\n",
    "    label = metrics.label_ids[i] #just to see positions where prompt tokens are at\n",
    "    logit_abcd = logit[label != IGNORE_INDEX]\n",
    "    toks = np.argmax(logit_abcd, axis=1)\n",
    "    print(\n",
    "        ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
