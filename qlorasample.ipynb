{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from qlora import *\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from packaging import version\n",
    "from packaging.version import parse\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from torchmetrics.functional.pairwise import pairwise_manhattan_distance as manhattan\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity as cossim\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    "\n",
    ")\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
    "from transformers.utils import is_peft_available\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "argdict = {\n",
    "  'model_name_or_path' : './mhllama', #'./mhllama',\n",
    "  'num_heads' : 4,\n",
    "  'data_seed' : 42 ,\n",
    "  'do_eval': True,\n",
    "  'eval_dataset_size' : 5 ,\n",
    "  'max_eval_samples' : 100 ,\n",
    "  'per_device_eval_batch_size' : 1 ,\n",
    "  'max_new_tokens' : 60 ,\n",
    "  'dataloader_num_workers' : 1 ,\n",
    "  'group_by_length' : True,\n",
    "  'remove_unused_columns' : False ,\n",
    "  'lora_r' : 64 ,\n",
    "  'lora_alpha' : 16 ,\n",
    "  'lora_modules' : 'all' ,\n",
    "  'double_quant' : True,\n",
    "  'quant_type' : 'nf4' ,\n",
    "  'bf16' : True,\n",
    "  'bits' : 4 ,\n",
    "  'dataset' : '/mnt/data/sonia/honeygan/apr23.dat',\n",
    "  'source_max_len' : 60 ,\n",
    "  'target_max_len' : 60 ,\n",
    "  'seed' : 0\n",
    "}\n",
    "\n",
    "arglist = [f'--{k}={v}' for k,v in argdict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfparser = transformers.HfArgumentParser((\n",
    "    ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
    "))\n",
    "model_args, data_args, training_args, generation_args  = hfparser.parse_args_into_dataclasses(args=arglist, return_remaining_strings=True)[:-1]\n",
    "training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading base model ./mhllama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of MultiheadLlamaForCausalLM were not initialized from the model checkpoint at ./mhllama and are newly initialized: ['heads.1.weight', 'heads.2.weight', 'heads.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MH llama has 4 heads\n",
      "Adding special tokens.\n",
      "adding LoRA modules...\n",
      "['heads.2', 'k_proj', 'heads.3', 'gate_proj', 'heads.1', 'v_proj', 'heads.0', 'o_proj', 'q_proj', 'up_proj', 'down_proj']\n",
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = get_accelerate_model(args, checkpoint_dir)\n",
    "model.config.use_cache = False\n",
    "    \n",
    "print('loaded model')\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a ', '<0x00> at IP ', '<0x00>, port ', '<0x00>, offering service ', '<0x00>']\n",
      "[tensor([263]), tensor([   3,  472, 5641]), tensor([    3, 29892,  2011]), tensor([    3, 29892, 27032,  2669]), tensor([3])]\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
    "collator = data_module['data_collator']\n",
    "datatr = data_module['train_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[  263,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     3,   472,  5641,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     3, 29892,  2011,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     3, 29892, 27032,  2669,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     3],\n",
       "          [  263,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     3,   472,  5641,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     3, 29892,  2011,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     3, 29892, 27032,  2669,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     3]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[ True, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False,  True,  True,  True,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False,  True,  True,  True, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False,  True,  True,  True,  True, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False,  True],\n",
       "          [ True, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False, False,  True,  True,  True,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False, False,  True,  True,  True, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False,  True,  True,  True,  True, False,\n",
       "           False, False, False, False, False, False, False, False, False, False,\n",
       "           False, False, False, False, False,  True]]),\n",
       "  'labels': tensor([[[ 5222,  3002,  8086, 19569,  8455,   313, 29881,  3844, 29897,     0,\n",
       "                0,     0,     0,     0,     0,     0],\n",
       "           [29871, 29953, 29900, 29889, 29906, 29945, 29900, 29889, 29947, 29946,\n",
       "            29889, 29929, 29900,     0,     0,     0],\n",
       "           [29871, 29947, 29900,     0,     0,     0,     0,     0,     0,     0,\n",
       "                0,     0,     0,     0,     0,     0],\n",
       "           [ 1732,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                0,     0,     0,     0,     0,     0]],\n",
       "  \n",
       "          [[ 2553,   713,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                0,     0,     0,     0,     0,     0],\n",
       "           [29871, 29896, 29947, 29947, 29889, 29896, 29953, 29953, 29889, 29906,\n",
       "            29941, 29929, 29889, 29896, 29900,     0],\n",
       "           [29871, 29946, 29946, 29941,     0,     0,     0,     0,     0,     0,\n",
       "                0,     0,     0,     0,     0,     0],\n",
       "           [ 2045,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "                0,     0,     0,     0,     0,     0]]])},\n",
       " tensor([  263,     3,   472,  5641,     3, 29892,  2011,     3, 29892, 27032,\n",
       "          2669,     3,   263,     3,   472,  5641,     3, 29892,  2011,     3,\n",
       "         29892, 27032,  2669,     3], dtype=torch.int32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = collator(datatr[0:2])\n",
    "out, out['input_ids'][out['attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainerclass = Seq2SeqTrainer\n",
    "trainer = trainerclass(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    ")\n",
    "class evalSampleCallback(transformers.TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        trainer.model.eval()\n",
    "        metrics = trainer.predict(test_dataset=data_module['eval_dataset'],metric_key_prefix=\"predict\")\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(metrics.predictions)):\n",
    "            logit = metrics.predictions[i]\n",
    "            label = metrics.label_ids[i] #just to see positions where prompt tokens are at\n",
    "            logit_abcd = logit[label != IGNORE_INDEX]\n",
    "            toks = np.argmax(logit_abcd, axis=1)\n",
    "            predictions.append(\n",
    "                ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "                )\n",
    "        \n",
    "        for pred in predictions:\n",
    "            print(pred)\n",
    "    \n",
    "    \n",
    "trainer.add_callback(evalSampleCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.set_trace(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'input_ids': torch.as_tensor([[    1, 29874,  3695,  1923,  7962,   472,  5641,  3695, 29892,  2011,\n",
    "          3695, 29892, 27032,   278,  2669,  3695, 29889,   263, 10829,  3002,\n",
    "         20579, 22814, 15629,   313,  8452, 29924, 29897, 29871, 29953, 29889,\n",
    "         29906, 29889, 29906, 29899, 29906, 29946, 29929, 29906, 29906,  1923,\n",
    "          7962,   472,  5641, 29871, 29896, 29906, 29896, 29889, 29906, 29906,\n",
    "         29929, 29889, 29896, 29929, 29941, 29889, 29896, 29900, 29946, 29892,\n",
    "          2011, 29871, 29945, 29900, 29900, 29900, 29892, 27032,   278,  2669,\n",
    "           274,   412,  8419, 29874, 29901,   335,   272]], device='cuda:0'),\n",
    "         'attention_mask': torch.as_tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True]], device='cuda:0'),\n",
    "         'labels': torch.as_tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
    "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   263, 10829,  3002,\n",
    "         20579, 22814, 15629,   313,  8452, 29924, 29897, 29871, 29953, 29889,\n",
    "         29906, 29889, 29906, 29899, 29906, 29946, 29929, 29906, 29906,  1923,\n",
    "          7962,   472,  5641, 29871, 29896, 29906, 29896, 29889, 29906, 29906,\n",
    "         29929, 29889, 29896, 29929, 29941, 29889, 29896, 29900, 29946, 29892,\n",
    "          2011, 29871, 29945, 29900, 29900, 29900, 29892, 27032,   278,  2669,\n",
    "           274,   412,  8419, 29874, 29901,   335,   272]], device='cuda:0')}\n",
    "out = trainer.model(**inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(out.hidden_states), out.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = out.hidden_states\n",
    "locs_batch, locs_tok = torch.where(inputs['input_ids']==3695)\n",
    "logits, indis = trainer.model.get_heads_logits(hidden_states, locs_tok[locs_batch==0], return_individual_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = inputs['labels'].cpu()\n",
    "logit_abcd = logits.float().detach().cpu()[label != IGNORE_INDEX]\n",
    "toks = np.argmax(logit_abcd, axis=1)\n",
    "print(\n",
    "    ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = inputs['labels'].cpu()\n",
    "for ho in indis:\n",
    "    logit_abcd = ho.detach().cpu()[label != IGNORE_INDEX]\n",
    "    toks = np.argmax(logit_abcd, axis=1)\n",
    "    print(\n",
    "        ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to load faster (work in progress, crashy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "if is_ipex_available() and torch.xpu.is_available():\n",
    "    n_gpus = torch.xpu.device_count()\n",
    "device_map='auto'\n",
    "max_memory = f'{args.max_memory_MB}MB'\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "compute_dtype = (torch.float16 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n",
    "\n",
    "transformers.AutoConfig.register('mhllama', MHLlamaConfig)\n",
    "transformers.AutoModelForCausalLM.register(MHLlamaConfig, MultiheadLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "sd_llamamodel = transformers.LlamaModel(transformers.LlamaConfig()).state_dict()\n",
    "headlist = []\n",
    "for i in range(4):\n",
    "    headlist.append( torch.nn.Linear(4096, 32000, bias=False) )\n",
    "heads = torch.nn.ModuleList(headlist)\n",
    "sd_heads = heads.state_dict()\n",
    "sd = OrderedDict(list(sd_llamamodel.items()) + list(sd_heads.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
