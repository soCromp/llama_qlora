{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from qlora import *\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import json\n",
    "from os.path import exists, join, isdir\n",
    "from dataclasses import dataclass, field\n",
    "import sys\n",
    "from typing import Optional, Dict, Sequence\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from packaging import version\n",
    "from packaging.version import parse\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from torchmetrics.functional.pairwise import pairwise_manhattan_distance as manhattan\n",
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity as cossim\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import argparse\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    set_seed,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer\n",
    "\n",
    ")\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import evaluate\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "from peft.tuners.lora import LoraLayer\n",
    "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
    "\n",
    "from transformers.modeling_utils import unwrap_model\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
    "from transformers.utils import is_peft_available\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "argdict = {\n",
    "  'model_name_or_path' : '/mnt/data/sonia/ckpts/diabetes-new/checkpoint-60', # './mhllama',\n",
    "  'num_heads': 10,\n",
    "  'max_column_len': 5,\n",
    "  'data_seed' : 42 ,\n",
    "  'do_eval': True,\n",
    "  'eval_dataset_size' : 5 ,\n",
    "  'max_eval_samples' : 2 ,\n",
    "  'per_device_eval_batch_size' : 1 ,\n",
    "  'dataloader_num_workers' : 1 ,\n",
    "  'group_by_length' : True,\n",
    "  'remove_unused_columns' : False ,\n",
    "  'lora_r' : 64 ,\n",
    "  'lora_alpha' : 16 ,\n",
    "  'lora_modules' : 'all' ,\n",
    "  'double_quant' : True,\n",
    "  'quant_type' : 'nf4' ,\n",
    "  'bf16' : True,\n",
    "  'bits' : 4 ,\n",
    "  'dataset' : '/mnt/data/sonia/datasets/diabetes/may10.dat',\n",
    "  'source_max_len' : 60 ,\n",
    "  'target_max_len' : 60 ,\n",
    "  'seed' : 0\n",
    "}\n",
    "\n",
    "arglist = [f'--{k}={v}' for k,v in argdict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hfparser = transformers.HfArgumentParser((\n",
    "    ModelArguments, DataArguments, TrainingArguments, GenerationArguments\n",
    "))\n",
    "model_args, data_args, training_args, generation_args  = hfparser.parse_args_into_dataclasses(args=arglist, return_remaining_strings=True)[:-1]\n",
    "training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
    "args = argparse.Namespace(\n",
    "    **vars(model_args), **vars(data_args), **vars(training_args)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading base model /mnt/data/sonia/ckpts/diabetes-new/checkpoint-60...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/configuration_utils.py:363: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MH llama has 10 heads\n",
      "Adding special tokens.\n",
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
    "model, tokenizer = get_accelerate_model(args, checkpoint_dir)\n",
    "model.config.use_cache = False\n",
    "    \n",
    "print('loaded model')\n",
    "set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The patient aged', ' received a', ' diagnosis after ', ' days in hospital with a', ' doctor where they underwent', ' procedures and were prescribed', ' medications. In the past year they had', ' emergency room visits', ' outpatient appointments and were ultimately', '']\n"
     ]
    }
   ],
   "source": [
    "data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
    "collator = data_module['data_collator']\n",
    "datatr = data_module['train_dataset']\n",
    "model.set_templates(collator.get_templates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers.integrations import WandbCallback\n",
    "trainerclass = Seq2SeqTrainer\n",
    "trainer = trainerclass(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
    ")\n",
    "class evalSampleCallback(transformers.TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, model, **kwargs):\n",
    "        trainer.model.eval()\n",
    "        metrics = trainer.predict(test_dataset=data_module['eval_dataset'],metric_key_prefix=\"predict\")\n",
    "        \n",
    "        predictions = []\n",
    "        for i in range(len(metrics.predictions)):\n",
    "            logit = metrics.predictions[i]\n",
    "            label = metrics.label_ids[i] #just to see positions where prompt tokens are at\n",
    "            logit_abcd = logit[label != IGNORE_INDEX]\n",
    "            toks = np.argmax(logit_abcd, axis=1)\n",
    "            predictions.append(\n",
    "                ''.join(trainer.tokenizer.decode(toks, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
    "                )\n",
    "        \n",
    "        for pred in predictions:\n",
    "            print(pred)\n",
    "            \n",
    "class WandbMetricsCallback(WandbCallback):\n",
    "    def on_substep_end(self, args, state, control, **kwargs):\n",
    "        self._wandb.log(model.to_log)\n",
    "    \n",
    "    \n",
    "trainer.add_callback(evalSampleCallback)\n",
    "trainer.add_callback(WandbMetricsCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.set_trace(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 1576, 16500, 26552,     0,     0,     0,     0,     0,     0,     3,\n",
       "          4520,   263,     0,     0,     0,     0,     0,     0,     3, 24876,\n",
       "         19263,  1156,     0,     0,     0,     0,     0,     0,     3,  3841,\n",
       "           297, 13457,   411,   263,     0,     0,     0,     0,     0,     0,\n",
       "             3, 11619,   988,   896,  1090, 29893,   296,     0,     0,     0,\n",
       "             0,     0,     0,     3, 28648,   322,   892,  2225, 23059,     0,\n",
       "             0,     0,     0,     0,     0,     3, 13589,   800, 29889,   512,\n",
       "           278,  4940,  1629,   896,   750,     0,     0,     0,     0,     0,\n",
       "             0,     3, 11176, 14703,  5716,  1998,  1169,     0,     0,     0,\n",
       "             0,     0,     0,     3,   714,  5031,   993,  8167,  1860,   322,\n",
       "           892, 18973,     0,     0,     0,     0,     0,     0,     3,     2]),\n",
       " tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 3, 3,\n",
       "         3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 5,\n",
       "         5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 0, 0]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prompt_template, model.head_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>The patient aged 20 to 5<0x00> received a Musculoskeletal<0x00> diagnosis after 3<unk><unk><unk><unk><0x00> days in hospital with a internal medicine doctor<unk><unk><unk><0x00> doctor where they underwent 1<unk><unk><unk><unk><0x00> procedures and were prescribed 15<unk><unk><unk><0x00> medications. In the past year they had 0<unk><unk><unk><unk><0x00> emergency room visits 0<unk><unk><unk><unk><0x00> outpatient appointments and were ultimately not readmitted<unk><unk><0x00></s>'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1,  1576, 16500, 26552, 29871, 29906, 29900,   304, 29871, 29945,\n",
    "             3,  4520,   263,  3077,  1810,   359,   446,  1026,   284,     3,\n",
    "         24876, 19263,  1156, 29871, 29941,     0,     0,     0,     0,     3,\n",
    "          3841,   297, 13457,   411,   263,  7463, 26602, 11619,     0,     0,\n",
    "             0,     3, 11619,   988,   896,  1090, 29893,   296, 29871, 29896,\n",
    "             0,     0,     0,     0,     3, 28648,   322,   892,  2225, 23059,\n",
    "         29871, 29896, 29945,     0,     0,     0,     3, 13589,   800, 29889,\n",
    "           512,   278,  4940,  1629,   896,   750, 29871, 29900,     0,     0,\n",
    "             0,     0,     3, 11176, 14703,  5716,  1998,  1169, 29871, 29900,\n",
    "             0,     0,     0,     0,     3,   714,  5031,   993,  8167,  1860,\n",
    "           322,   892, 18973,   451,  1303, 29885,  4430,     0,     0,     3,\n",
    "             2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in forward. input_ids tensor([[    1,  1576, 16500, 26552, 29871, 29906, 29900,   304, 29871, 29945,\n",
      "             3,  4520,   263,  3077,  1810,   359,   446,  1026,   284,     3,\n",
      "         24876, 19263,  1156, 29871, 29941,     0,     0,     0,     0,     3,\n",
      "          3841,   297, 13457,   411,   263,  7463, 26602, 11619,     0,     0,\n",
      "             0,     3, 11619,   988,   896,  1090, 29893,   296, 29871, 29896,\n",
      "             0,     0,     0,     0,     3, 28648,   322,   892,  2225, 23059,\n",
      "         29871, 29896, 29945,     0,     0,     0,     3, 13589,   800, 29889,\n",
      "           512,   278,  4940,  1629,   896,   750, 29871, 29900,     0,     0,\n",
      "             0,     0,     3, 11176, 14703,  5716,  1998,  1169, 29871, 29900,\n",
      "             0,     0,     0,     0,     3,   714,  5031,   993,  8167,  1860,\n",
      "           322,   892, 18973,   451,  1303, 29885,  4430,     0,     0,     3,\n",
      "             2]], device='cuda:0') label tensor([[    1,  1576, 16500, 26552, 29871, 29906, 29900,   304, 29871, 29945,\n",
      "             3,  4520,   263,  3077,  1810,   359,   446,  1026,   284,     3,\n",
      "         24876, 19263,  1156, 29871, 29941,     0,     0,     0,     0,     3,\n",
      "          3841,   297, 13457,   411,   263,  7463, 26602, 11619,     0,     0,\n",
      "             0,     3, 11619,   988,   896,  1090, 29893,   296, 29871, 29896,\n",
      "             0,     0,     0,     0,     3, 28648,   322,   892,  2225, 23059,\n",
      "         29871, 29896, 29945,     0,     0,     0,     3, 13589,   800, 29889,\n",
      "           512,   278,  4940,  1629,   896,   750, 29871, 29900,     0,     0,\n",
      "             0,     0,     3, 11176, 14703,  5716,  1998,  1169, 29871, 29900,\n",
      "             0,     0,     0,     0,     3,   714,  5031,   993,  8167,  1860,\n",
      "           322,   892, 18973,   451,  1303, 29885,  4430,     0,     0,     3,\n",
      "             2]], device='cuda:0')\n",
      "logits argmax tensor([[ 1576, 16500, 26552,     0,     0,     0,     0,     0,     0, 29900,\n",
      "          4520,   263,     0,     0,     0,     0,     0,     0,     3, 24876,\n",
      "         19263,  1156,     0,     0,     0,     0,     0,     0,     3,  3841,\n",
      "           297, 13457,   411,   263,     0,     0,     0,     0,     0,     0,\n",
      "             3, 11619,   988,   896,  1090, 29893,   296,     0,     0,     0,\n",
      "             0,     0,     0,     3, 28648,   322,   892,  2225, 23059,     0,\n",
      "             0,     0,     0,     0,     0,     3, 13589,   800, 29889,   512,\n",
      "           278,  4940,  1629,   896,   750,     0,     0,     0,     0,     0,\n",
      "             0,     3, 11176, 14703,  5716,  1998,  1169,     0,     0,     0,\n",
      "             0,     0,     0,     3,   714,  5031,   993,  8167,  1860,   322,\n",
      "           892, 18973,     0,     0,     0,     0,     0,     0,     3,     2,\n",
      "             0]], device='cuda:0')\n",
      "logits argmax tensor([[ 1576, 16500, 26552,   451, 29945, 29900,  2440,  1303, 29946, 29900,\n",
      "          4520,   263,     0,     0,     0,     0,     0,     0,     3, 24876,\n",
      "         19263,  1156,     0,     0,     0,     0,     0,     0,     3,  3841,\n",
      "           297, 13457,   411,   263,     0,     0,     0,     0,     0,     0,\n",
      "             3, 11619,   988,   896,  1090, 29893,   296,     0,     0,     0,\n",
      "             0,     0,     0,     3, 28648,   322,   892,  2225, 23059,     0,\n",
      "             0,     0,     0,     0,     0,     3, 13589,   800, 29889,   512,\n",
      "           278,  4940,  1629,   896,   750,     0,     0,     0,     0,     0,\n",
      "             0,     3, 11176, 14703,  5716,  1998,  1169,     0,     0,     0,\n",
      "             0,     0,     0,     3,   714,  5031,   993,  8167,  1860,   322,\n",
      "           892, 18973,     0,     0,     0,     0,     0,     0,     3,     2,\n",
      "             0]], device='cuda:0')\n",
      "logits argmax tensor([[ 1576, 16500, 26552,   451, 29945, 29900,  2440,  1303, 29946, 29900,\n",
      "          4520,   263, 29871,  1810,   359,   446,     3,   284,     3, 24876,\n",
      "         19263,  1156,     0,     0,     0,     0,     0,     0,     3,  3841,\n",
      "           297, 13457,   411,   263,     0,     0,     0,     0,     0,     0,\n",
      "             3, 11619,   988,   896,  1090, 29893,   296,     0,     0,     0,\n",
      "             0,     0,     0,     3, 28648,   322,   892,  2225, 23059,     0,\n",
      "             0,     0,     0,     0,     0,     3, 13589,   800, 29889,   512,\n",
      "           278,  4940,  1629,   896,   750,     0,     0,     0,     0,     0,\n",
      "             0,     3, 11176, 14703,  5716,  1998,  1169,     0,     0,     0,\n",
      "             0,     0,     0,     3,   714,  5031,   993,  8167,  1860,   322,\n",
      "           892, 18973,     0,     0,     0,     0,     0,     0,     3,     2,\n",
      "             0]], device='cuda:0')\n",
      "logits argmax tensor([[ 1576, 16500, 26552,   451, 29945, 29900,  2440,  1303, 29946, 29900,\n",
      "          4520,   263, 29871,  1810,   359,   446,     3,   284,     3, 24876,\n",
      "         19263,  1156,   512, 29896,     0,     0,     0,     0,     3,  3841,\n",
      "           297, 13457,   411,   263,     0,     0,     0,     0,     0,     0,\n",
      "             3, 11619,   988,   896,  1090, 29893,   296,     0,     0,     0,\n",
      "             0,     0,     0,     3, 28648,   322,   892,  2225, 23059,     0,\n",
      "             0,     0,     0,     0,     0,     3, 13589,   800, 29889,   512,\n",
      "           278,  4940,  1629,   896,   750,     0,     0,     0,     0,     0,\n",
      "             0,     3, 11176, 14703,  5716,  1998,  1169,     0,     0,     0,\n",
      "             0,     0,     0,     3,   714,  5031,   993,  8167,  1860,   322,\n",
      "           892, 18973,     0,     0,     0,     0,     0,     0,     3,     2,\n",
      "             0]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1781\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1782\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1783\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1784\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1785\u001b[0m     )\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/trainer.py:2118\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2118\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2121\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2122\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2123\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2124\u001b[0m ):\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/trainer.py:3036\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3036\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[1;32m   3038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3039\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/trainer.py:3059\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m   3060\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3062\u001b[0m \u001b[38;5;66;03m# print('trainer compute_loss self.args.past_index', self.args.past_index)\u001b[39;00m\n\u001b[1;32m   3063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# always -1\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[0;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 813 (3 times), autocast_decorator.<locals>.decorate_autocast at line 14 (3 times), convert_outputs_to_fp32.<locals>.forward at line 825 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/utils/operations.py:825\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/utils/operations.py:813\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llama-qlora/multihead_models.py:457\u001b[0m, in \u001b[0;36mMultiheadLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, col, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    455\u001b[0m loss_cum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads):\n\u001b[0;32m--> 457\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m    458\u001b[0m         column\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    459\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    460\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    461\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    462\u001b[0m         past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    463\u001b[0m         inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    464\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    465\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    466\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    467\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    468\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    469\u001b[0m     )\n\u001b[1;32m    470\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    472\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# batch_size x tokens x 4096\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llama-qlora/multihead_models.py:207\u001b[0m, in \u001b[0;36mMOELlamaModel.forward\u001b[0;34m(self, column, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    204\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m--> 207\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    208\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    209\u001b[0m         hidden_states,\n\u001b[1;32m    210\u001b[0m         column,\n\u001b[1;32m    211\u001b[0m         causal_mask,\n\u001b[1;32m    212\u001b[0m         position_ids,\n\u001b[1;32m    213\u001b[0m         past_key_values,\n\u001b[1;32m    214\u001b[0m         output_attentions,\n\u001b[1;32m    215\u001b[0m         use_cache,\n\u001b[1;32m    216\u001b[0m         cache_position,\n\u001b[1;32m    217\u001b[0m     )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    220\u001b[0m         hidden_states,\n\u001b[1;32m    221\u001b[0m         column,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    228\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/utils/checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected keyword arguments: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m kwargs))\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CheckpointFunction\u001b[38;5;241m.\u001b[39mapply(function, preserve, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    252\u001b[0m         function,\n\u001b[1;32m    253\u001b[0m         preserve,\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/utils/checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m run_function(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/llama-qlora/multihead_models.py:96\u001b[0m, in \u001b[0;36mMOELlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, column, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m     97\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m     98\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     99\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    100\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    101\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    102\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    103\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/models/llama/modeling_llama.py:353\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    352\u001b[0m     key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 353\u001b[0m     value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n\u001b[1;32m    355\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    356\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:288\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_layer(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# sure.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:468\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    465\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    467\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 468\u001b[0m out \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt(), bias\u001b[38;5;241m=\u001b[39mbias, quant_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mquant_state)\n\u001b[1;32m    470\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatMul4Bit\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, quant_state)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, F\u001b[38;5;241m.\u001b[39mdequantize_4bit(B, quant_state)\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    512\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The patient aged', ' received a', ' diagnosis after ', ' days in hospital with a', ' where they underwent', ' procedures and were prescribed', ' medications. In the past year they had', ' emergency room visits', ' outpatient appointments and were ultimately', '']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/mnt/data/zoo/llama2/llama2-7b-hf/',\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False, # Fast tokenizer giving issues.\n",
    "        )\n",
    "data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
    "collator = data_module['data_collator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.AutoConfig.register('mhllama', MHLlamaConfig)\n",
    "transformers.AutoModelForCausalLM.register(MHLlamaConfig, MultiheadLlamaForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m            MHLlamaConfig\n",
      "\u001b[0;31mFile:\u001b[0m            ~/llama-qlora/multihead_models.py\n",
      "\u001b[0;31mDocstring:\u001b[0m       <no docstring>\n",
      "\u001b[0;31mClass docstring:\u001b[0m\n",
      "This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n",
      "model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
      "defaults will yield a similar configuration to that of the LLaMA-7B.\n",
      "\n",
      "Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
      "documentation from [`PretrainedConfig`] for more information.\n",
      "\n",
      "\n",
      "Args:\n",
      "    vocab_size (`int`, *optional*, defaults to 32000):\n",
      "        Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n",
      "        `inputs_ids` passed when calling [`LlamaModel`]\n",
      "    hidden_size (`int`, *optional*, defaults to 4096):\n",
      "        Dimension of the hidden representations.\n",
      "    intermediate_size (`int`, *optional*, defaults to 11008):\n",
      "        Dimension of the MLP representations.\n",
      "    num_hidden_layers (`int`, *optional*, defaults to 32):\n",
      "        Number of hidden layers in the Transformer decoder.\n",
      "    num_attention_heads (`int`, *optional*, defaults to 32):\n",
      "        Number of attention heads for each attention layer in the Transformer decoder.\n",
      "    num_key_value_heads (`int`, *optional*):\n",
      "        This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
      "        `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
      "        `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
      "        converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
      "        by meanpooling all the original heads within that group. For more details checkout [this\n",
      "        paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
      "        `num_attention_heads`.\n",
      "    hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
      "        The non-linear activation function (function or string) in the decoder.\n",
      "    max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
      "        The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n",
      "        Llama 2 up to 4096, CodeLlama up to 16384.\n",
      "    initializer_range (`float`, *optional*, defaults to 0.02):\n",
      "        The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
      "    rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
      "        The epsilon used by the rms normalization layers.\n",
      "    use_cache (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
      "        relevant if `config.is_decoder=True`.\n",
      "    pad_token_id (`int`, *optional*):\n",
      "        Padding token id.\n",
      "    bos_token_id (`int`, *optional*, defaults to 1):\n",
      "        Beginning of stream token id.\n",
      "    eos_token_id (`int`, *optional*, defaults to 2):\n",
      "        End of stream token id.\n",
      "    pretraining_tp (`int`, *optional*, defaults to 1):\n",
      "        Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
      "        document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n",
      "        necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
      "        issue](https://github.com/pytorch/pytorch/issues/76232).\n",
      "    tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
      "        Whether to tie weight embeddings\n",
      "    rope_theta (`float`, *optional*, defaults to 10000.0):\n",
      "        The base period of the RoPE embeddings.\n",
      "    rope_scaling (`Dict`, *optional*):\n",
      "        Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
      "        strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
      "        `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
      "        `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
      "        these scaling strategies behave:\n",
      "        https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
      "        experimental feature, subject to breaking API changes in future versions.\n",
      "    attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
      "        Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
      "    attention_dropout (`float`, *optional*, defaults to 0.0):\n",
      "        The dropout ratio for the attention probabilities.\n",
      "\n",
      "```python\n",
      ">>> from transformers import LlamaModel, LlamaConfig\n",
      "\n",
      ">>> # Initializing a LLaMA llama-7b style configuration\n",
      ">>> configuration = LlamaConfig()\n",
      "\n",
      ">>> # Initializing a model from the llama-7b style configuration\n",
      ">>> model = LlamaModel(configuration)\n",
      "\n",
      ">>> # Accessing the model configuration\n",
      ">>> configuration = model.config\n",
      "```"
     ]
    }
   ],
   "source": [
    "config?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.72 GiB total capacity; 15.38 GiB already allocated; 6.56 MiB free; 15.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m MHLlamaConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mvars\u001b[39m(args))\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      3\u001b[0m             args\u001b[38;5;241m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m      4\u001b[0m             config \u001b[38;5;241m=\u001b[39m config, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/modeling_utils.py:3603\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3600\u001b[0m     model\u001b[38;5;241m.\u001b[39mhf_quantizer \u001b[38;5;241m=\u001b[39m hf_quantizer\n\u001b[1;32m   3602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3603\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_adapter(\n\u001b[1;32m   3604\u001b[0m         _adapter_model_path,\n\u001b[1;32m   3605\u001b[0m         adapter_name\u001b[38;5;241m=\u001b[39madapter_name,\n\u001b[1;32m   3606\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   3607\u001b[0m         adapter_kwargs\u001b[38;5;241m=\u001b[39madapter_kwargs,\n\u001b[1;32m   3608\u001b[0m     )\n\u001b[1;32m   3610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   3611\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/transformers-4.39.3/src/transformers/integrations/peft.py:193\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_peft_config_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     adapter_state_dict \u001b[38;5;241m=\u001b[39m load_peft_weights(peft_model_id, token\u001b[38;5;241m=\u001b[39mtoken, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39madapter_kwargs)\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# We need to pre-process the state dict to remove unneeded prefixes - for backward compatibility\u001b[39;00m\n\u001b[1;32m    196\u001b[0m processed_adapter_state_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/peft/utils/save_and_load.py:270\u001b[0m, in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find weights for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or in the Hugging Face Hub. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAFETENSORS_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is present at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m             )\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_safetensors:\n\u001b[0;32m--> 270\u001b[0m     adapters_weights \u001b[38;5;241m=\u001b[39m safe_load_file(filename, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m     adapters_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(filename, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(device))\n",
      "File \u001b[0;32m/mnt/data/sonia/miniconda3/envs/llama/lib/python3.11/site-packages/safetensors/torch.py:310\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(filename, device)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m safe_open(filename, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 310\u001b[0m         result[k] \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mget_tensor(k)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.72 GiB total capacity; 15.38 GiB already allocated; 6.56 MiB free; 15.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "config = MHLlamaConfig(**vars(args))\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            config = config, device_map='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model.set_templates(collator.get_templates())\n",
    "model = PeftModel.from_pretrained(model, join(args.model_name_or_path, 'adapter_model'), is_trainable=True)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_trace(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = collator(1*[{'length': 0}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 0, 0,\n",
       "        0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0,\n",
       "        0, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 0, 0, 0, 0, 0,\n",
       "        0, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.head_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 2])\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[   1, 1576]]) tensor([[0, 1]])\n",
      "about to generate token 2 with head 0\n",
      "in forward. input_ids tensor([[   1, 1576]]) label None\n",
      "logits argmax tensor([[ 1576, 16500]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[16500]])\n",
      "input ids update tensor([[    1,  1576, 16500]])\n",
      "3 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 3])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[16500]]) tensor([[2]])\n",
      "about to generate token 3 with head 0\n",
      "in forward. input_ids tensor([[16500]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[26552]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552]])\n",
      "4 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 4])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[26552]]) tensor([[3]])\n",
      "about to generate token 4 with head 1\n",
      "in forward. input_ids tensor([[26552]]) label None\n",
      "logits argmax tensor([[451]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[451]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451]])\n",
      "5 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 5])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[451]]) tensor([[4]])\n",
      "about to generate token 5 with head 1\n",
      "in forward. input_ids tensor([[451]]) label None\n",
      "logits argmax tensor([[1303]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[1303]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303]])\n",
      "6 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 6])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[1303]]) tensor([[5]])\n",
      "about to generate token 6 with head 1\n",
      "in forward. input_ids tensor([[1303]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0]])\n",
      "7 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 7])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[6]])\n",
      "about to generate token 7 with head 1\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0]])\n",
      "8 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 8])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[7]])\n",
      "about to generate token 8 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213]])\n",
      "9 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 9])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[6213]]) tensor([[8]])\n",
      "about to generate token 9 with head 0\n",
      "in forward. input_ids tensor([[6213]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[4520]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520]])\n",
      "10 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 10])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[4520]]) tensor([[9]])\n",
      "about to generate token 10 with head 0\n",
      "in forward. input_ids tensor([[4520]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[263]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263]])\n",
      "11 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 11])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[263]]) tensor([[10]])\n",
      "about to generate token 11 with head 2\n",
      "in forward. input_ids tensor([[263]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0]])\n",
      "12 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 12])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[11]])\n",
      "about to generate token 12 with head 2\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0]])\n",
      "13 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 13])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[12]])\n",
      "about to generate token 13 with head 2\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0]])\n",
      "14 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 14])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[13]])\n",
      "about to generate token 14 with head 2\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0]])\n",
      "15 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 15])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[14]])\n",
      "about to generate token 15 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213]])\n",
      "16 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 16])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[6213]]) tensor([[15]])\n",
      "about to generate token 16 with head 0\n",
      "in forward. input_ids tensor([[6213]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[24876]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876]])\n",
      "17 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 17])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[24876]]) tensor([[16]])\n",
      "about to generate token 17 with head 0\n",
      "in forward. input_ids tensor([[24876]]) label None\n",
      "logits argmax tensor([[19263]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[19263]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263]])\n",
      "18 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 18])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[19263]]) tensor([[17]])\n",
      "about to generate token 18 with head 0\n",
      "in forward. input_ids tensor([[19263]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[1156]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156]])\n",
      "19 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 19])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[1156]]) tensor([[18]])\n",
      "about to generate token 19 with head 3\n",
      "in forward. input_ids tensor([[1156]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0]])\n",
      "20 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 20])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[19]])\n",
      "about to generate token 20 with head 3\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0]])\n",
      "21 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 21])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[20]])\n",
      "about to generate token 21 with head 3\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0]])\n",
      "22 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 22])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[21]])\n",
      "about to generate token 22 with head 3\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0]])\n",
      "23 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 23])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[22]])\n",
      "about to generate token 23 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213]])\n",
      "24 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 24])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[6213]]) tensor([[23]])\n",
      "about to generate token 24 with head 0\n",
      "in forward. input_ids tensor([[6213]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[3841]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841]])\n",
      "25 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 25])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[3841]]) tensor([[24]])\n",
      "about to generate token 25 with head 0\n",
      "in forward. input_ids tensor([[3841]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[297]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297]])\n",
      "26 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 26])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[297]]) tensor([[25]])\n",
      "about to generate token 26 with head 0\n",
      "in forward. input_ids tensor([[297]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[13457]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457]])\n",
      "27 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 27])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[13457]]) tensor([[26]])\n",
      "about to generate token 27 with head 0\n",
      "in forward. input_ids tensor([[13457]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[411]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411]])\n",
      "28 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 28])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[411]]) tensor([[27]])\n",
      "about to generate token 28 with head 0\n",
      "in forward. input_ids tensor([[411]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[263]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263]])\n",
      "29 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 29])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[263]]) tensor([[28]])\n",
      "about to generate token 29 with head 4\n",
      "in forward. input_ids tensor([[263]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0]])\n",
      "30 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 30])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[29]])\n",
      "about to generate token 30 with head 4\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0]])\n",
      "31 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 31])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[30]])\n",
      "about to generate token 31 with head 4\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0]])\n",
      "32 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 32])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[31]])\n",
      "about to generate token 32 with head 4\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0]])\n",
      "33 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 33])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[32]])\n",
      "about to generate token 33 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213]])\n",
      "34 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 34])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[6213]]) tensor([[33]])\n",
      "about to generate token 34 with head 0\n",
      "in forward. input_ids tensor([[6213]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[11619]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619]])\n",
      "35 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 35])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[11619]]) tensor([[34]])\n",
      "about to generate token 35 with head 0\n",
      "in forward. input_ids tensor([[11619]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[988]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988]])\n",
      "36 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 36])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[988]]) tensor([[35]])\n",
      "about to generate token 36 with head 0\n",
      "in forward. input_ids tensor([[988]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[896]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896]])\n",
      "37 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 37])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[896]]) tensor([[36]])\n",
      "about to generate token 37 with head 0\n",
      "in forward. input_ids tensor([[896]]) label None\n",
      "logits argmax tensor([[1090]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[1090]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090]])\n",
      "38 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 38])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[1090]]) tensor([[37]])\n",
      "about to generate token 38 with head 0\n",
      "in forward. input_ids tensor([[1090]]) label None\n",
      "logits argmax tensor([[29893]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[29893]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893]])\n",
      "39 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 39])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[29893]]) tensor([[38]])\n",
      "about to generate token 39 with head 0\n",
      "in forward. input_ids tensor([[29893]]) label None\n",
      "logits argmax tensor([[296]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[296]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296]])\n",
      "40 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 40])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[296]]) tensor([[39]])\n",
      "about to generate token 40 with head 5\n",
      "in forward. input_ids tensor([[296]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0]])\n",
      "41 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 41])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[40]])\n",
      "about to generate token 41 with head 5\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0]])\n",
      "42 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 42])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[41]])\n",
      "about to generate token 42 with head 5\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0]])\n",
      "43 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 43])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[42]])\n",
      "about to generate token 43 with head 5\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0]])\n",
      "44 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 44])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[43]])\n",
      "about to generate token 44 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213]])\n",
      "45 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 45])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[6213]]) tensor([[44]])\n",
      "about to generate token 45 with head 0\n",
      "in forward. input_ids tensor([[6213]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[28648]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648]])\n",
      "46 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 46])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[28648]]) tensor([[45]])\n",
      "about to generate token 46 with head 0\n",
      "in forward. input_ids tensor([[28648]]) label None\n",
      "logits argmax tensor([[322]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[322]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322]])\n",
      "47 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 47])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[322]]) tensor([[46]])\n",
      "about to generate token 47 with head 0\n",
      "in forward. input_ids tensor([[322]]) label None\n",
      "logits argmax tensor([[892]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[892]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892]])\n",
      "48 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 48])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[892]]) tensor([[47]])\n",
      "about to generate token 48 with head 0\n",
      "in forward. input_ids tensor([[892]]) label None\n",
      "logits argmax tensor([[2225]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[2225]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225]])\n",
      "49 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 49])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[2225]]) tensor([[48]])\n",
      "about to generate token 49 with head 0\n",
      "in forward. input_ids tensor([[2225]]) label None\n",
      "logits argmax tensor([[23059]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[23059]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059]])\n",
      "50 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 50])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[23059]]) tensor([[49]])\n",
      "about to generate token 50 with head 6\n",
      "in forward. input_ids tensor([[23059]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0]])\n",
      "51 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 51])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[50]])\n",
      "about to generate token 51 with head 6\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0]])\n",
      "52 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 52])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[51]])\n",
      "about to generate token 52 with head 6\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0]])\n",
      "53 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 53])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[52]])\n",
      "about to generate token 53 with head 6\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0]])\n",
      "54 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 54])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[53]])\n",
      "about to generate token 54 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213]])\n",
      "55 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 55])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[6213]]) tensor([[54]])\n",
      "about to generate token 55 with head 0\n",
      "in forward. input_ids tensor([[6213]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[13589]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589]])\n",
      "56 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 56])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[13589]]) tensor([[55]])\n",
      "about to generate token 56 with head 0\n",
      "in forward. input_ids tensor([[13589]]) label None\n",
      "logits argmax tensor([[800]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[800]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800]])\n",
      "57 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 57])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[800]]) tensor([[56]])\n",
      "about to generate token 57 with head 0\n",
      "in forward. input_ids tensor([[800]]) label None\n",
      "logits argmax tensor([[29889]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[29889]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889]])\n",
      "58 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 58])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[29889]]) tensor([[57]])\n",
      "about to generate token 58 with head 0\n",
      "in forward. input_ids tensor([[29889]]) label None\n",
      "logits argmax tensor([[512]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[512]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512]])\n",
      "59 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 59])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[512]]) tensor([[58]])\n",
      "about to generate token 59 with head 0\n",
      "in forward. input_ids tensor([[512]]) label None\n",
      "logits argmax tensor([[278]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[278]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278]])\n",
      "60 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 60])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[278]]) tensor([[59]])\n",
      "about to generate token 60 with head 0\n",
      "in forward. input_ids tensor([[278]]) label None\n",
      "logits argmax tensor([[4940]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[4940]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940]])\n",
      "61 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 61])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[4940]]) tensor([[60]])\n",
      "about to generate token 61 with head 0\n",
      "in forward. input_ids tensor([[4940]]) label None\n",
      "logits argmax tensor([[1629]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[1629]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629]])\n",
      "62 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 62])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[1629]]) tensor([[61]])\n",
      "about to generate token 62 with head 0\n",
      "in forward. input_ids tensor([[1629]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[896]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896]])\n",
      "63 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 63])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[896]]) tensor([[62]])\n",
      "about to generate token 63 with head 0\n",
      "in forward. input_ids tensor([[896]]) label None\n",
      "logits argmax tensor([[750]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[750]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750]])\n",
      "64 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 64])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[750]]) tensor([[63]])\n",
      "about to generate token 64 with head 7\n",
      "in forward. input_ids tensor([[750]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0]])\n",
      "65 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 65])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[64]])\n",
      "about to generate token 65 with head 7\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0]])\n",
      "66 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 66])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[65]])\n",
      "about to generate token 66 with head 7\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0]])\n",
      "67 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 67])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[66]])\n",
      "about to generate token 67 with head 7\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0]])\n",
      "68 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 68])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[67]])\n",
      "about to generate token 68 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213]])\n",
      "69 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 69])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[6213]]) tensor([[68]])\n",
      "about to generate token 69 with head 0\n",
      "in forward. input_ids tensor([[6213]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[11176]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176]])\n",
      "70 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 70])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[11176]]) tensor([[69]])\n",
      "about to generate token 70 with head 0\n",
      "in forward. input_ids tensor([[11176]]) label None\n",
      "logits argmax tensor([[14703]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[14703]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703]])\n",
      "71 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 71])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[14703]]) tensor([[70]])\n",
      "about to generate token 71 with head 0\n",
      "in forward. input_ids tensor([[14703]]) label None\n",
      "logits argmax tensor([[5716]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[5716]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716]])\n",
      "72 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 72])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[5716]]) tensor([[71]])\n",
      "about to generate token 72 with head 0\n",
      "in forward. input_ids tensor([[5716]]) label None\n",
      "logits argmax tensor([[1998]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[1998]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998]])\n",
      "73 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 73])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[1998]]) tensor([[72]])\n",
      "about to generate token 73 with head 0\n",
      "in forward. input_ids tensor([[1998]]) label None\n",
      "logits argmax tensor([[1169]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[1169]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169]])\n",
      "74 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 74])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[1169]]) tensor([[73]])\n",
      "about to generate token 74 with head 8\n",
      "in forward. input_ids tensor([[1169]]) label None\n",
      "logits argmax tensor([[29871]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[29871]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871]])\n",
      "75 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 75])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[29871]]) tensor([[74]])\n",
      "about to generate token 75 with head 8\n",
      "in forward. input_ids tensor([[29871]]) label None\n",
      "logits argmax tensor([[29900]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[29900]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900]])\n",
      "76 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 76])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[29900]]) tensor([[75]])\n",
      "about to generate token 76 with head 8\n",
      "in forward. input_ids tensor([[29900]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0]])\n",
      "77 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 77])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[76]])\n",
      "about to generate token 77 with head 8\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0]])\n",
      "78 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 78])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[77]])\n",
      "about to generate token 78 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213]])\n",
      "79 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 79])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[6213]]) tensor([[78]])\n",
      "about to generate token 79 with head 0\n",
      "in forward. input_ids tensor([[6213]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[714]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714]])\n",
      "80 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 80])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[714]]) tensor([[79]])\n",
      "about to generate token 80 with head 0\n",
      "in forward. input_ids tensor([[714]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[5031]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031]])\n",
      "81 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 81])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[5031]]) tensor([[80]])\n",
      "about to generate token 81 with head 0\n",
      "in forward. input_ids tensor([[5031]]) label None\n",
      "logits argmax tensor([[993]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[993]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993]])\n",
      "82 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 82])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[993]]) tensor([[81]])\n",
      "about to generate token 82 with head 0\n",
      "in forward. input_ids tensor([[993]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[8167]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167]])\n",
      "83 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 83])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[8167]]) tensor([[82]])\n",
      "about to generate token 83 with head 0\n",
      "in forward. input_ids tensor([[8167]]) label None\n",
      "logits argmax tensor([[1860]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[1860]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860]])\n",
      "84 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 84])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[1860]]) tensor([[83]])\n",
      "about to generate token 84 with head 0\n",
      "in forward. input_ids tensor([[1860]]) label None\n",
      "logits argmax tensor([[322]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[322]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860,   322]])\n",
      "85 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 85])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[322]]) tensor([[84]])\n",
      "about to generate token 85 with head 0\n",
      "in forward. input_ids tensor([[322]]) label None\n",
      "logits argmax tensor([[892]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[892]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860,   322,   892]])\n",
      "86 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 86])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[892]]) tensor([[85]])\n",
      "about to generate token 86 with head 0\n",
      "in forward. input_ids tensor([[892]]) label None\n",
      "logits argmax tensor([[18973]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[18973]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860,   322,   892, 18973]])\n",
      "87 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 87])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[18973]]) tensor([[86]])\n",
      "about to generate token 87 with head 9\n",
      "in forward. input_ids tensor([[18973]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860,   322,   892, 18973,     0]])\n",
      "88 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 88])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[87]])\n",
      "about to generate token 88 with head 9\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860,   322,   892, 18973,     0,     0]])\n",
      "89 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 89])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[88]])\n",
      "about to generate token 89 with head 9\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860,   322,   892, 18973,     0,     0,     0]])\n",
      "90 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 90])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[89]])\n",
      "about to generate token 90 with head 9\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[0]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860,   322,   892, 18973,     0,     0,     0,\n",
      "             0]])\n",
      "91 cur_len\n",
      "in prepare inputs for generation with args\n",
      "torch.Size([1, 91])\n",
      "<class 'tuple'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'NoneType'>\n",
      "<class 'torch.Tensor'>\n",
      "{'use_cache': True}\n",
      "model_inputs tensor([[0]]) tensor([[90]])\n",
      "about to generate token 91 with head 0\n",
      "in forward. input_ids tensor([[0]]) label None\n",
      "logits argmax tensor([[0]])\n",
      "next token scores torch.Size([1, 32000])\n",
      "tensor([[6213]])\n",
      "input ids update tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
      "           263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
      "             0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
      "             0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
      "             0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
      "             0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
      "          4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
      "         14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
      "          5031,   993,  8167,  1860,   322,   892, 18973,     0,     0,     0,\n",
      "             0,  6213]])\n",
      "92 cur_len\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,  1576, 16500, 26552,   451,  1303,     0,     0,  6213,  4520,\n",
       "            263,     0,     0,     0,     0,  6213, 24876, 19263,  1156,     0,\n",
       "              0,     0,     0,  6213,  3841,   297, 13457,   411,   263,     0,\n",
       "              0,     0,     0,  6213, 11619,   988,   896,  1090, 29893,   296,\n",
       "              0,     0,     0,     0,  6213, 28648,   322,   892,  2225, 23059,\n",
       "              0,     0,     0,     0,  6213, 13589,   800, 29889,   512,   278,\n",
       "           4940,  1629,   896,   750,     0,     0,     0,     0,  6213, 11176,\n",
       "          14703,  5716,  1998,  1169, 29871, 29900,     0,     0,  6213,   714,\n",
       "           5031,   993,  8167,  1860,   322,   892, 18973,     0,     0,     0,\n",
       "              0,  6213]]),\n",
       " tensor([[[  451,  1303,     0,     0],\n",
       "          [    0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0],\n",
       "          [    0,     0,     0,     0],\n",
       "          [29871, 29900,     0,     0],\n",
       "          [    0,     0,     0,     0]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(**inputs)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>The patient aged not read<unk><unk> None received a<unk><unk><unk><unk> None diagnosis after<unk><unk><unk><unk> None days in hospital with a<unk><unk><unk><unk> None doctor where they underwent<unk><unk><unk><unk> None procedures and were prescribed<unk><unk><unk><unk> None medications. In the past year they had<unk><unk><unk><unk> None emergency room visits 0<unk><unk> None outpatient appointments and were ultimately<unk><unk><unk><unk> None']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cos distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>native-country</th>\n",
       "      <th>education</th>\n",
       "      <th>occupation</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>Male</td>\n",
       "      <td>United States</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Protective serv</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>Male</td>\n",
       "      <td>United States</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Other service</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>Exec managerial</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>Male</td>\n",
       "      <td>United States</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>Sales</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>Male</td>\n",
       "      <td>United States</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Sales</td>\n",
       "      <td>over 50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age     sex native-country     education       occupation     income\n",
       "0  22    Male  United States  Some-college  Protective serv  under 50K\n",
       "1  36    Male  United States       HS-grad    Other service  under 50K\n",
       "2  29  Female  United States     Assoc-voc  Exec managerial  under 50K\n",
       "3  18    Male  United States       HS-grad            Sales  under 50K\n",
       "4  67    Male  United States  Some-college            Sales   over 50K"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "full_dataset = DatasetDict({})\n",
    "for f in os.listdir(args.dataset):\n",
    "    if f.endswith('.json'): continue\n",
    "    full_dataset[f] = load_from_disk(os.path.join(args.dataset, f))\n",
    "real = full_dataset['train'].to_pandas().drop(['length'], axis=1)\n",
    "real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared_logits_processor []\n",
      "prepared_stopping_criteria [<transformers.generation.stopping_criteria.MaxLengthCriteria object at 0x7f180e5794d0>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared_logits_processor []\n",
      "prepared_stopping_criteria [<transformers.generation.stopping_criteria.MaxLengthCriteria object at 0x7f1876610a50>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared_logits_processor []\n",
      "prepared_stopping_criteria [<transformers.generation.stopping_criteria.MaxLengthCriteria object at 0x7f1876d3bf10>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared_logits_processor []\n",
      "prepared_stopping_criteria [<transformers.generation.stopping_criteria.MaxLengthCriteria object at 0x7f180f252110>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared_logits_processor []\n",
      "prepared_stopping_criteria [<transformers.generation.stopping_criteria.MaxLengthCriteria object at 0x7f180e5b5650>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>5th-6th</td>\n",
       "      <td>Tech support</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>Protective serv</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Transport moving</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Protective serv</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>Female</td>\n",
       "      <td>United States</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>Exec managerial</td>\n",
       "      <td>under 50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0       1              2             3                 4          5\n",
       "0  62  Female  United States       5th-6th      Tech support  under 50K\n",
       "1  41  Female  United States       7th-8th   Protective serv  under 50K\n",
       "2  55  Female  United States     Bachelors  Transport moving  under 50K\n",
       "3  47  Female  United States  Some-college   Protective serv  under 50K\n",
       "4  38  Female  United States  Some-college   Exec managerial  under 50K"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [ [] for _ in range(real.shape[1]) ]\n",
    "batch_size = 50\n",
    "num_samples = 225 # real.shape[0]\n",
    "inputs = collator(batch_size*[{'length': 0}])\n",
    "\n",
    "for batch in range(225//50 + 1):\n",
    "    _, batch_col_toks = model.generate(**inputs) # batch_size x num_cols x max_column_len\n",
    "\n",
    "    for i, col in enumerate(real.columns):\n",
    "        options_str = real[col].unique()\n",
    "        options = tokenizer(options_str.tolist(), add_special_tokens=False, padding='max_length', return_tensors='pt', \n",
    "                            max_length=args.generation_config.max_column_len, truncation=True)['input_ids']\n",
    "        preds_col = options_str[cosine_similarity(batch_col_toks[:, i, :], options).argmax(axis=1)]\n",
    "        preds[i].extend(preds_col)\n",
    "\n",
    "preds = pd.DataFrame(preds).T\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 6)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp = datasets.Dataset.from_pandas(preds)\n",
    "hp.save_to_disk(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf5ec876a7745a0b66153977c3cef1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonia/transformers-4.39.3/src/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/sonia/transformers-4.39.3/src/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/sonia/transformers-4.39.3/src/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sonia/transformers-4.39.3/src/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"/mnt/data/zoo/llama2/llama2-7b-hf/\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/data/zoo/llama2/llama2-7b-hf/\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared_logits_processor []\n",
      "prepared_stopping_criteria [<transformers.generation.stopping_criteria.MaxLengthCriteria object at 0x7f5db05c9ad0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hey, are you conscious? Can you talk to me? I’m a doctor. I'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate\n",
    "generate_ids = model.generate(inputs.input_ids, do_sample=True, num_beams=1)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
